{"meta":{"title":"King   James","subtitle":"筚路蓝缕，以启山林","description":"stay hungry foolish and curious","author":"Mr.Yi","url":"http://github.com"},"pages":[{"title":"about","date":"2018-04-13T07:50:16.000Z","updated":"2018-04-13T08:20:10.135Z","comments":false,"path":"about/index.html","permalink":"http://github.com/about/index.html","excerpt":"","text":"Alongside the benefits, AI will also bring dangers, like powerful autonomous weapons, or new ways for the few to oppress the many."},{"title":"分类","date":"2018-04-13T07:29:19.000Z","updated":"2018-04-13T07:42:40.521Z","comments":false,"path":"categories/index.html","permalink":"http://github.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-04-13T07:45:37.000Z","updated":"2018-04-13T07:47:09.308Z","comments":false,"path":"tags/index.html","permalink":"http://github.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hexo + GitHub Pages 搭建个人博客","slug":"Hexo + GitHub Pages 搭建个人博客","date":"2018-04-13T14:08:49.000Z","updated":"2018-04-13T14:09:53.864Z","comments":true,"path":"2018/04/13/Hexo + GitHub Pages 搭建个人博客/","link":"","permalink":"http://github.com/2018/04/13/Hexo + GitHub Pages 搭建个人博客/","excerpt":"Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds. Documentationhexo.io 这一篇 note 讲解如何使用 Hexo + Github Pages 搭建个人博客，并用 GitHub 进行版本控制。其中源文件位于 hexo 分支，静态文件位于 master 分支。 准备 安装最新版的 Git。在命令行输入 git version 检查 git 是否安装成功。 安装 LTS 版的 Node.js。同样在命令行输入 node -v 和 npm -v以检查 node.js 是否安装成功。 注册 GitHub 账号，新建一个 repository，一般命名为 username.github.io，这样 GitHub 会自动开启 GitHub Pages 功能。勾选 Initialize this repository with a README 的话即可访问个人主页，否则需要添加内容才能访问，建议暂时不要勾选。 GitHub 添加 SSH（推荐），可参考 Connecting to GitHub with SSH。","text":"Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds. Documentationhexo.io 这一篇 note 讲解如何使用 Hexo + Github Pages 搭建个人博客，并用 GitHub 进行版本控制。其中源文件位于 hexo 分支，静态文件位于 master 分支。 准备 安装最新版的 Git。在命令行输入 git version 检查 git 是否安装成功。 安装 LTS 版的 Node.js。同样在命令行输入 node -v 和 npm -v以检查 node.js 是否安装成功。 注册 GitHub 账号，新建一个 repository，一般命名为 username.github.io，这样 GitHub 会自动开启 GitHub Pages 功能。勾选 Initialize this repository with a README 的话即可访问个人主页，否则需要添加内容才能访问，建议暂时不要勾选。 GitHub 添加 SSH（推荐），可参考 Connecting to GitHub with SSH。 Hexo安装 Hexo1npm install hexo-cli -g 使用 hexo version 检查是否安装成功。 建站选择一个目录，比如 D:\\github，键入以下命令： 123hexo init username.github.iocd username.github.ionpm install 这样就创建了一个名为 username.github.io 的 Hexo 工程（文件夹）。注意，hexo init &lt;folder&gt; 命令要求 folder 为空文件夹，否则会报错。 启动服务器1hexo server (简写为 hexo s) 默认情况下，访问网址为 http://localhost:4000/。打开网址，可以看到一篇 landscape 主题的 Hello World 博客。一般修改 Markdown 文件，不需要重启服务器，直接刷新浏览器即可，除非你修改配置文件。 修改配置站点配置工程根目录下的 _config.yml 称为站点配置文件，可以配置一些个人信息等，具体可参考Configuration。 主题配置每个主题的目录下也会有一个 _config.yml 文件，称为主题配置文件。Hexo 有丰富多彩的主题，这里以 hexo-theme-hiker 为例，说明如何更换主题。 安装主题： 12cd username.github.iogit clone git@github.com:iTimeTraveler/hexo-theme-hiker.git themes/hiker PS：这样安装主题并不能 push 到 GitHub 中去，可使用 fork + subtree 的方法解决，具体参考 Hexo 主题同步。感谢 @Tyrion Yu 的帮助。 修改站点配置文件，将 theme 修改为 hiker： # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ theme: hiker 重启服务器，即可查看效果。在某些情况（尤其是更换主题后），如果发现对站点的更改无论如何也不生效，可能需要 clean 一下。 12hexo cleanhexo server PS：如果不需要 landscape 主题，直接删除 themes 下的对应文件夹即可。 部署配置安装 hexo-deployer-git。 1npm install hexo-deployer-git --save 然后修改站点配置文件： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:username/username.github.io.git # 这种配置需使用 SSH branch: master message: message # 默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }} 其中 branch 为静态文件所在的分支，必须为 master 分支。message 表示自定义提交信息，一般不需要配置，删除该行即可。 GitGit 初始化为工程创建 Git 仓库： 12cd username.github.iogit init 创建分支此时 Git 仓库为空，不能直接运行 git branch hexo 来创建新的分支，可通过以下命令创建： 1git checkout -b hexo push 源文件添加所有文件，提交到本地仓库： 12git add .git commit -m \"first commit\" 添加远程仓库，并push： 12git remote add origin git@github.com:username.github.io.gitgit push -u origin hexo 部署静态文件先生成静态文件，再部署： 123hexo cleanhexo generate （简写为 hexo g）hexo delpoy (简写为 hexo d) 此时整个部署过程就结束啦，你可以通过 https://username.github.io/ 访问自己的 Github Pages。 推荐阅读 How to use Hexo and deploy to GitHub Pages Hexo Documentation 知乎：使用hexo，如果换了电脑怎么更新博客？ An attractive theme for Hexo. called “Hiker”, short for “HikerNews” Hexo 主题同步 使用 git subtree 集成项目到子目录 Git subtree: the alternative to Git submodule","categories":[{"name":"blog","slug":"blog","permalink":"http://github.com/categories/blog/"}],"tags":[{"name":"Git笔记","slug":"Git笔记","permalink":"http://github.com/tags/Git笔记/"}]},{"title":"Git--命令(二)","slug":"Git--命令(二)","date":"2018-04-13T14:04:29.000Z","updated":"2018-04-13T14:06:01.044Z","comments":true,"path":"2018/04/13/Git--命令(二)/","link":"","permalink":"http://github.com/2018/04/13/Git--命令(二)/","excerpt":"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Documentationgit-scm.com Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。它是由 Linux 之父 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。本文介绍了 Git 的常用命令。 三种状态在学习 Git 命令之前，首先要理解它的三种状态：已提交（committed）、已修改（modified）和已暂存（staged）。已提交表示数据已经安全的保存在本地数据库中；已修改表示修改了文件，但还没保存到数据库中，增加、删除文件也相当于已修改；已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库、工作目录以及暂存区域。","text":"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Documentationgit-scm.com Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。它是由 Linux 之父 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。本文介绍了 Git 的常用命令。 三种状态在学习 Git 命令之前，首先要理解它的三种状态：已提交（committed）、已修改（modified）和已暂存（staged）。已提交表示数据已经安全的保存在本地数据库中；已修改表示修改了文件，但还没保存到数据库中，增加、删除文件也相当于已修改；已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库、工作目录以及暂存区域。 它们之间的关系可以参考 Git 工作区、暂存区和版本库。 在阅读下面的内容之前，最好在自己的电脑上安装 Git，然后按照顺序操作。如果你想先感受一下 Git 的魅力， Try Git 是一个不错的选择。 Git 配置安装完 Git，初次运行前需要做一些配置，比如用户信息： 12git config --global user.name \"Your Name\"git config --global user.email email@example.com Windows 环境下，推荐使用文本编辑器 Notepad++： 12# 注意更改为自己的安装目录git config --global core.editor \"'C:\\Program Files\\Notepad++\\notepad++.exe' -multiInst -nosession\" 配置完成后，可以通过 git config --list 查看所有的配置信息，或者使用 git config user.name 查看单个信息。 另外，还可以自定义配置一些命令的别名，方便记忆。 1git config --global alias.last 'log -1' 这样 git last 就相当于 git log -1，用于查看最后一次的提交记录。我比较喜欢这样配置，用于查看提交历史： 1git config --global alias.lg \"log --oneline --decorate --graph --all\" 创建版本库创建版本库有两种方式，一种是使用 git clone 从现有 Git 仓库中拷贝项目，格式如下： 1git clone &lt;repo&gt; &lt;directory&gt; 另一种是通过 git init 初始化一个 Git 仓库，省略 directory 会在当前文件夹中创建。 1git init &lt;directory&gt; 例如，在 D:\\test 文件夹下执行 git init 命令，这样会生成一个 隐藏 的 .git 目录。 12$ git initInitialized empty Git repository in D:/test/.git/ Git 工作流程基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 使用 Git 时文件的生命周期如下： 上图来源于 Pro Git，这里的 Add the file 应该理解为使用 git add 命令，Reomve the file 则是手动删除文件。 First Commit在 D:\\test 中手动添加 a.txt 文件，使用 Notepad++ 编辑（不要用记事本），然后运行 git status 命令，查看当前状态： 1234567891011$ git statusOn branch masterNo commits yetUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) a.txtnothing added to commit but untracked files present (use \"git add\" to track) Git 的提示十分人性化，可以看出 a.txt 处于 Untracked 状态。执行 git add 命令： 1234567891011$ git add a.txt$ git statusOn branch masterNo commits yetChanges to be committed: (use \"git rm --cached &lt;file&gt;...\" to unstage) new file: a.txt 此时 a.txt 处于 Staged 状态，可以通过 git rm --cached &lt;file&gt;... 使其回到 Untracked 状态。最后执行 git commit 命令，进行第一次提交。 1234$ git commit -m \"Add a.txt\"[master (root-commit) 3fbc25c] Add a.txt 1 file changed, 1 insertion(+) create mode 100644 a.txt 其中 -m 是参数，后面跟着提交信息。如果配置了文本编辑器，执行不带参数的 git commit 后，可在弹出的编辑器中填写提交信息。注意只有 保存文件 并 退出编辑器，commit 才会生效。 另外，在只 修改文件 时，使用 -a 可以跳过 Staged 状态直接提交，可以和 -m 一起使用： 1git commit -am \"Update file\" Second Commit添加 b.txt，然后修改 a.txt，查看此时的状态： 1234567891011121314$ git statusOn branch masterChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: a.txtUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) b.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") 此时 a.txt 处于 Modified 状态，可通过 git checkout -- &lt;file&gt;... 放弃更改，但是要 慎用，这些更改是找不回来的。 而 b.txt 处于 Untracked 状态。 git diff 命令用于比较工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 git diff --cached（Git 1.6.1 及更高版本还允许使用 git diff --staged，效果是相同的，但更好记些）可以查看已暂存的将要添加到下次提交里的内容。 123456789$ git diffdiff --git a/a.txt b/a.txtindex 69dd9b9..b0c1f18 100644--- a/a.txt+++ b/a.txt@@ -1 +1,2 @@ aaaaaaaaaa # a.txt 原本的内容+AAAAAAAAAA # a.txt 添加的内容$ git diff --staged # nothing 添加这两个文件到暂存区： 123456789$ git add \"*.txt\" # git add .(一个点，表示添加所有文件)$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) modified: a.txt new file: b.txt 同理，git reset HEAD &lt;file&gt;... 命令可使文件回到 add 之前的状态。此时再次执行 diff： 1234567891011121314151617$ git diff # nothing$ git diff --stageddiff --git a/a.txt b/a.txtindex 69dd9b9..b0c1f18 100644--- a/a.txt+++ b/a.txt@@ -1 +1,2 @@ aaaaaaaaaa+AAAAAAAAAAdiff --git a/b.txt b/b.txtnew file mode 100644index 0000000..817e5ca--- /dev/null+++ b/b.txt@@ -0,0 +1 @@+bbbbbbbbbb # b.txt 中添加的内容 以上对比可以看出不同 diff 的差别。执行 commit 命令进行第二次提交： 1234$ git commit -m \"Update a.txt and add b.txt\"[master 24e0903] Update a.txt and add b.txt 2 files changed, 2 insertions(+) create mode 100644 b.txt 删除为了演示删除操作，先添加 c.txt： 123456$ git add c.txt$ git commit -m \"Add c.txt\"[master 9d8751a] Add c.txt 1 file changed, 1 insertion(+) create mode 100644 c.txt 手动删除后的状态为 Untracked。 12345678$ git statusChanges not staged for commit: (use \"git add/rm &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) deleted: c.txtno changes added to commit (use \"git add\" and/or \"git commit -a\") 使用 git checkout -- &lt;file&gt;... 撤销，然后执行 git rm 命令，此时的状态为 Staged。这就是两者的差别吧。 12345678$ git rm c.txtrm 'c.txt'$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) deleted: c.txt 提交删除： 1234$ git commit -m \"Delete c.txt\"[master 95d6e7e] Delete c.txt 1 file changed, 1 deletion(-) delete mode 100644 c.txt 如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除 git rm -f &lt;file&gt;。 这是一种安全特性，用于防止误删还没有添加到快照的数据，这样的数据不能被 Git 恢复。 而 git rm --cached 命令只会将文件从 Git 仓库中删除，但仍然保留在当前工作目录中。当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆无关的文件添加到暂存区时，这一做法尤其有用。 版本回退在 Git 中，用 HEAD 表示当前版本，上一个版本就是 HEAD^（HEAD~）。有关 ~ 和 ^ 的区别，请参考 What’s the difference between HEAD^ and HEAD~ in Git? 注意，Windows 环境下 ^ 识别不了，必须加 双引号 才行，像这样 &quot;HEAD^&quot;。 假如又要用到 c.txt，想反悔，怎么办？Git 允许我们在版本的历史之间穿梭，使用 git reset --hard &lt;commit_id&gt; 命令。如果不知道 commit_id，git log 可以查看提交历史。 12345678910111213$ git lg # 自定义的 git log* 95d6e7e (HEAD -&gt; master) Delete c.txt* 9d8751a Add c.txt* 24e0903 Update a.txt and add b.txt* 3fbc25c Add a.txt$ git reset --hard HEAD~HEAD is now at 9d8751a Add c.txt$ git lg* 9d8751a (HEAD -&gt; master) Add c.txt* 24e0903 Update a.txt and add b.txt* 3fbc25c Add a.txt 其中 3fbc25c 为版本号（commit_id），它是一个由 SHA-1 计算出来的校验和，用十六进制表示，而且每次都不一样。因为我使用了自定义的 git lg， 这里只显示 7 位，其实它是 3fbc25c7d58e06169a45b587a9c6164234efd43c。 git log 功能十分强大，可参考 Git Basics - Viewing the Commit History。 另外，可以使用命令 git reflog 查看命令历史。如果想回到 Delete c.txt 的版本，直接 reset 对应的 commit_id 即可。 123456$ git reflog9d8751a (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to HEAD~95d6e7e HEAD@&#123;1&#125;: commit: Delete c.txt9d8751a HEAD@&#123;2&#125;: commit: Add c.txt24e0903 HEAD@&#123;3&#125;: commit: Update a.txt and add b.txt3fbc25c HEAD@&#123;4&#125;: commit (initial): Add a.txt Reset其实 reset 分三类，分别为 --soft、--mixed（默认，可不加）和 --hard，它们之间到底有什么区别呢？我们做个试验。注意此时是 Add c.txt 的版本。 soft12345678$ git reset --soft HEAD~$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: c.txt --soft 参数使文件回到了 Staged 的状态。 mixed重新回到 Add c.txt 的版本，执行 --mixed 命令： 12345678910$ git reset --mixed HEAD~$ git statusOn branch masterUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) c.txtnothing added to commit but untracked files present (use \"git add\" to track) --mixed 参数使文件回到了 Untracked 状态。 hard重新回到 Add c.txt 的版本，执行 --hard 命令： 12345$ git reset --hard HEAD~HEAD is now at 24e0903 Update a.txt and add b.txt$ git statusOn branch masternothing to commit, working tree clean 而 --hard 参数直接回到了上一个版本。 想要了解更多关于 Reset 的知识，请参考 Git Tools - Reset Demystified。 Git 分支几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。有人把 Git 的分支模型称为它的“必杀技特性”，也正因为这一特性，使得 Git 从众多版本控制系统中脱颖而出。为何 Git 的分支模型如此出众呢？Git 处理分支的方式可谓是难以置信的轻量，创建新分支这一操作几乎能在瞬间完成，并且在不同分支之间的切换操作也是一样便捷。 下面演示了 Git 分支的工作流程。创建并切换到 dev 分支： 1234$ git branch dev$ git checkout devSwitched to branch 'dev' 简单地，这两个命令可以合并为一个命令： 12$ git checkout -b devSwitched to a new branch 'dev' 在 dev 分支添加 d.txt，修改 c.txt，提交： 123456$ git add .$ git commit -m \"Add d.txt and update c.txt\"[dev 7f5d2b1] Add d.txt and update c.txt 2 files changed, 2 insertions(+), 1 deletion(-) create mode 100644 d.txt 切换到 master 分支，合并 dev 分支： 123456789$ git checkout master$ git merge devUpdating 9d8751a..7f5d2b1Fast-forward c.txt | 2 +- d.txt | 1 + 2 files changed, 2 insertions(+), 1 deletion(-) create mode 100644 d.txt 最后删除 dev 分支： 12$ git branch -d devDeleted branch dev (was 7f5d2b1). Git 远程仓库为了能在任意 Git 项目上协作，需要知道如何管理自己的远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库。 你可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。 这里以 GitHub 为例，演示如何使用远程仓库。在 GitHub 上创建一个新的 Repository，不要添加任何内容，完成后如下图所示： 添加远程仓库： 1git remote add origin git@github.com:muwednesday/git-learning.git 使用命令 git push 将本地仓库推送到 GitHub，其中 -u 为设置当前本地分支的默认远程分支。 12345678910$ git push -u origin masterCounting objects: 14, done.Delta compression using up to 8 threads.Compressing objects: 100% (7/7), done.Writing objects: 100% (14/14), 913 bytes | 304.00 KiB/s, done.Total 14 (delta 2), reused 0 (delta 0)remote: Resolving deltas: 100% (2/2), done.To github.com:muwednesday/git-learning.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'. 刷新页面后即可看到文件。然后在 GitHub 上创建一个 README.md 的文件，提交。 返回本地仓库，查看状态，这里居然显示 up to date。本来应该落后才对，为什么呢？原因参见 Why does git status show branch is up-to-date when changes exist upstream? 12345$ git statusOn branch masterYour branch is up to date with 'origin/master'.nothing to commit, working tree clean 最后使用命令 git pull 来自动的抓取然后合并远程分支到当前分支。 123456789101112$ git pullremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:muwednesday/git-learning 7f5d2b1..ad27849 master -&gt; origin/masterUpdating 7f5d2b1..ad27849Fast-forward README.md | 1 + 1 file changed, 1 insertion(+) create mode 100644 README.md 推荐阅读 Pro Git GitHub Cheat Sheet Reference Manual 廖雪峰的 Git 教程 What’s the difference between HEAD^ and HEAD~ in Git? Reset, Checkout, and Revert What is the difference between ‘git pull’ and ‘git fetch’?","categories":[{"name":"工具","slug":"工具","permalink":"http://github.com/categories/工具/"}],"tags":[{"name":"git","slug":"git","permalink":"http://github.com/tags/git/"}]},{"title":"hiker博客主题配置","slug":"hiker博客主题配置","date":"2018-04-13T02:01:03.000Z","updated":"2018-04-13T04:34:32.000Z","comments":true,"path":"2018/04/13/hiker博客主题配置/","link":"","permalink":"http://github.com/2018/04/13/hiker博客主题配置/","excerpt":"HikerAn attractive, exquisite theme for Hexo. named “Hiker”, short for “HikerNews”. ☞ 在线预览 | Hiker问题交流群","text":"HikerAn attractive, exquisite theme for Hexo. named “Hiker”, short for “HikerNews”. ☞ 在线预览 | Hiker问题交流群 以上Demo站点的源文件在这里，大家有需要的可以参考：https://github.com/iTimeTraveler/hexo-theme-hiero/tree/site-source 安装步骤 从GitHub上获取代码 1$ git clone https://github.com/iTimeTraveler/hexo-theme-hiker.git themes/hiker 启用 把Hexo主目录下的 _config.yml 文件中的字段 theme 修改为 hiker. 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: hiker 更新 12$ cd themes/Hiker$ git pull 特性自定义首页背景您可以将选择的大图放到 YOUR_HEXO_SITE\\themes\\hiker\\source\\css\\images 文件夹下. 然后更改 hiker/_config.yml文件里的home_background_image字段. 12345678# Homepage# eg. home_background_image: [css/images/home-bg.jpg, http://t.cn/RMbvEza]# eg. mode: image | polyline | trianglifyhome_background_image: enable: true mode: image rolling: true url: [css/images/home-bg.jpg, css/images/sample.jpg, https://source.unsplash.com/collection/954550/1920x1080] 首页背景填充方式有三种可选mode： image: 大图模式 trianglify: 多边形渐变背景 polyline: 随机彩色折线 默认配置为image模式，也就是大图模式。多边形渐变背景trianglify模式来自Trianglify大致如下图： 如果你不中意以上两种背景填充方式，可以选择随机彩色折线polyline模式，长相参考下图。 ！！注意：如果在使用image模式时url为空（enable仍然保持true）, 主题也会自动使用下面这种漂亮的随机线条 填充（也就是会自动退化为polyline模式）： Code 色彩主题Hiker 使用Tomorrow Theme 作为代码高亮的配色. 总共有六种选择: default, normal, night, night blue, night bright, night eighties 默认高亮配色如上图。 另外的五种配色如下. Modify highlight_theme in hiker/_config.yml. 12345# Code Highlight theme# Available value:# default | normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: default 博客主题色Hiker 为你的博客提供了五种可选的主题色，可以配置成random, 每次生成博客时会自动随机使用一个主题色. orange blue red green black You can modify theme_color in hiker/_config.yml. 1234# Article theme color# Available value:# random | orange | blue | red | green | blacktheme_color: random 夜间模式只有在文章阅读页面，点击左上角的logo图片，就能打开设置对话框，操作如下图 站内搜索Hiker 使用 Insight Search 实现了站内搜索，在_config.yml文件中启用如下. 12345# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false ！注意: 在使用搜索功能前必须在Hexo目录下使用以下命令安装 hexo-generator-json-content 插件. 1$ npm install -S hexo-generator-json-content FancyboxHiker使用Fancybox来浏览展示您文章中的图片，支持以下方式在文章中添加图片： 123![img caption](img url)&#123;% fancybox img_url [img_thumbnail] [img_caption] %&#125; 侧边栏sidebar（侧边栏位置）可以设置为 left , right, bottom. Hiker 有以下5种侧边栏插件: category tag tagcloud archives recent_posts All of them are enabled by default. You can edit them in widget setting. 打赏捐赠按钮 每篇文章最后显示打赏按钮，目前仅支持微信支付和支付宝两种打赏方式。您可以在文件 hiker/_config.yml 中配置您的微信、支付宝付款二维码图片的URL: 123456# donation buttondonate: enable: true message: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!' wechatImage: https://your_WECHAT_PAY_ImageUrl alipayImage: https://your_ALIPAY_ImageUrl 评论已完全支持原生Disqus、livere（来必力）、wumii（无觅）评论系统。因多说、网易云跟帖均已停止服务，在国内建议大家使用相对稳定的来必力评论系统。在文件 hiker/_config.yml 中修改以下代码片段: 1234567# comment ShortName, you can choose only ONE to display.gentie_productKey: #网易云跟帖your-gentie-product-keyduoshuo_shortname:disqus_shortname:livere_shortname: MTAyMC8yOTQ4MS82MDQ5uyan_uid:wumii: 网易云跟帖说明（已停止服务） 登陆 网易云跟帖 获取你的 Product Key。请注意，您在云跟帖管理后台设置的域名必须跟您站点的域名一致。在本地测试时，需要做两步骤前置设定： 修改 hosts 文件，将您域名的请求指向本地。例如：127.0.0.1 yoursite.com 修改 Hexo 监听的端口为 80：hexo s --debug -p 80 测试完成后请将 hosts 文件中的域名映射删除即可。 支持的浏览器 Contributing欢迎大家有各种问题和改进建议的，直接提issue或者评论，或者pull request都行。我会尽量抽时间和大家交流。刚接触Hexo不久，疏忽不足之处，还望大家海涵！","categories":[{"name":"blog","slug":"blog","permalink":"http://github.com/categories/blog/"}],"tags":[{"name":"blog config","slug":"blog-config","permalink":"http://github.com/tags/blog-config/"}]},{"title":"背包问题","slug":"背包问题","date":"2018-04-12T16:57:50.000Z","updated":"2018-04-13T07:38:39.541Z","comments":true,"path":"2018/04/13/背包问题/","link":"","permalink":"http://github.com/2018/04/13/背包问题/","excerpt":"0/1背包问题 有N件物品和一个容量为V的背包。放入第i件物品耗费的空间是Ci，得到的价值是Wi。求解将哪些物品装入背包可使价值总和最大？ 将每一件物品从1到n编号，从第1件物品开始，每一件物品就只有两个状态：放进背包了 / 没有放进背包。 我们画一张表格，行对应着每一件物品，列对应着背包的重量，那么pack[i][j]就表示 前i件物品，背包最大承重j 这个子问题的解。","text":"0/1背包问题 有N件物品和一个容量为V的背包。放入第i件物品耗费的空间是Ci，得到的价值是Wi。求解将哪些物品装入背包可使价值总和最大？ 将每一件物品从1到n编号，从第1件物品开始，每一件物品就只有两个状态：放进背包了 / 没有放进背包。 我们画一张表格，行对应着每一件物品，列对应着背包的重量，那么pack[i][j]就表示 前i件物品，背包最大承重j 这个子问题的解。 给一组数据作为样例： 5 10 6 2 3 2 5 6 4 5 6 4 第一行表示有5件物品，10为最大承重，2-6行为5个物品的价值和重量。 生成以下的表格 0 6 6 6 6 6 6 6 6 6 0 6 6 9 9 9 9 9 9 9 0 6 6 9 9 9 9 11 11 14 0 6 6 9 9 9 10 11 13 14 0 6 6 9 9 12 12 15 15 15 所以最终的结果是最后一行最后一列的 15 给出代码： 12345678910111213141516171819202122232425262728293031/* 01 package problem */#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;int pack[100][1000];int c[100],w[100];void make(int n, int r)&#123; memset(pack,0,sizeof(pack)); for (int i = 1; i &lt;= n; i++) for (int j = w[i]; j &lt;= r; j++) pack[i][j] = max(pack[i - 1][j - c[i]] + w[i], pack[i - 1][j]); cout&lt;&lt;pack[n][r];&#125;int main()&#123; int t,n,V; cin&gt;&gt;t; while(t--)&#123; //多组数据 cin&gt;&gt;n&gt;&gt;V; for (int i = 1; i &lt;= n; i++) cin&gt;&gt;c[i]&gt;&gt;w[i]; make(n,V); &#125; return 0;&#125; 代码优化 这个代码在时间上应该已经不能再优化了，但是还可以考虑空间复杂度的优化。 优化的基本思路： 考虑所用到的状态转移方程: pack[i][j] = max(pack[i-1][j-c[i]] + w[i], pack[i-1][j]); 可以发现 pack[i][j] 的值并不和整个二维表的每一个数字的值都有关，而是仅仅和上面一行的值有关，所以可以使用 pack[2][n] 这么大的数组来存储结果。 考虑状态转移方程的实际情况，还可以使用一维数组来进行运算，但是要注意的是，此时，循环应该从后往前进行。因为如果是按从前往后的顺序，那么 pack[i][j] = max(pack[i][j-c[i]] + w[i] , pack[i][j]); 中进行比较的两个值 pack[i][j] 是没有更新的，也就是 pack[i-1][j] 的值，而 pack[i][j - c[i]]一定是前面被更新过的，也就是 pack[i][j-w[i]] 的值。这就是说，max() 比较的两个数是属于原来二维数组中不同的两行，而不是我们期望的相同的两行。 如果上面的说法不能理解我们不妨这样：有一件物品的性价比很高，在pack数组的某个位置，我们第一次将这个物品放入背包中，但是按照从前往后的顺序，很可能在这个位置的后面某个位置我们会再次将这个物品添加进去。 优化后的代码 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;int pack[10000],c[1000],w[1000];void make(int n, int r)&#123; memset(pack,0,sizeof(pack)); for (int i = 1; i &lt;= n; i++) for (int j = r; j &gt;= w[i]; j--) pack[j] = max(pack[j], pack[j - c[i]] + w[i]); cout&lt;&lt;pack[r]&lt;&lt;endl;&#125;int main()&#123; int n,t,V; cin&gt;&gt;t; while(t--)&#123; cin&gt;&gt;n&gt;&gt;V; for (int i = 1; i &lt;= n; i++) cin&gt;&gt;c[i]&gt;&gt;w[i]; make(n,V); &#125; return 0;&#125; 初始化问题： 如果限定背包必须装满，那么需要将数组初始化为 -∞ （负无穷大） 如果背包可以不装满，那么数组初始化为0 为了后面的书写方便，我们把代码改成这样1234void ZeroOnePack(int c,int w)&#123; for (int i = V; i &gt;= c; i--) pack[i] = max(pack[i], pack[i - c] + w);&#125; 这样01背包问题的主要代码就是这样： 12for (int i = 0; i &lt; n; i++) ZeroOnePack(c[i],w[i]); 这样ZeroOnePack()这个函数就专门解决了“放一个物品”的问题 完全背包问题 完全背包问题和0/1背包问题几乎一模一样，不同的就是物品不再是一个，而是无数个 思路 完全背包不同处是原来的一个物品变成了无数个，但是我们还是可以把它变成0/1背包问题的，试想一下，即使拥有无数个物品，但是真的可以用无数个吗？ 不可能，因为背包的容量有限，所以每个物品c,w最多可以使用[V/c]个，所以以下面的数据为例： c: 3 2 5 4 w: 7 4 2 5 V = 10 我们完全可以把这组数据改成这样： c: 3 3 3 2 2 2 2 2 5 5 4 4 w: 7 7 7 4 4 4 4 4 2 2 5 5 原因自然是背包容量最大为10,所以占用空间为3的物品最多放3个，修改过后的数据就可以用0/1背包的方法处理 那难道完全背包需要重开一个c2[],w2[]，然后按0/1背包处理吗？ 当然不是，还记得我们将0/1背包进行优化时说的如果循环从前向后进行会发生什么后果吗？ 这一句 “但是按照从前往后的顺序，很可能在这个位置的后面某个位置我们会再次将这个物品添加进去。” 看到了？0/1背包时为了避免重复，我们将循环改为从后往前，但是完全背包是可以重复使用物品的，对吧？所以代码： 1234void CompletePack(c,w)&#123; for (int i = c; i &lt;= V; i++) pack[i] = max(pack[i],pack[i - c] + w )&#125; 怎么样，和0/1背包只有一点点的差别对不对？ 3.多重背包问题 多重背包和0/1背包不同的地方就是物品不是一个而是有m个 所以我们还是就一个物品c,w,m分析： 对于m可能有两种情况： m &gt;= [V/c]，这种情况明显是完全背包 0 &lt; m &lt; [v/c]，对于这种情况需要认真分析一下 我们仍然需要按照0/1背包的思路把这些物品拆开，而且我们要保证我们拆出来的这些物品可以通过组合表示出1到m任意件物。 我们可以考虑二进制的计数方法，这样我们把物品拆成(c,w) , (2c,2w) , (4c,4w) …… [(m-2^k)*c , (m-2^k)*w)] 不管最优解会在这件物品中取几件，我们都可以用我们拆出来的这些物品来表示（请自己证明，二进制的思想） 所以，有了思路，代码就简单了： 12345678910111213void MultiplePack(c,w,m)&#123; if (c * m &gt;= V) &#123; CompletePack(c,w); return; &#125; k = 1; while (k &lt; m) &#123; ZeroOnePack(c*k,w*k); m = m - k; k = 2 * k; &#125; ZeroOnePack(c * m, w * m);&#125; 其实就是0/1背包和完全背包的组合，有木有？","categories":[{"name":"算法","slug":"算法","permalink":"http://github.com/categories/算法/"}],"tags":[{"name":"背包问题","slug":"背包问题","permalink":"http://github.com/tags/背包问题/"},{"name":"算法","slug":"算法","permalink":"http://github.com/tags/算法/"},{"name":"C/C+","slug":"C-C","permalink":"http://github.com/tags/C-C/"}]},{"title":"Markdown文档中mathjax的问题","slug":"markdown文档中mathjax的问题","date":"2018-04-12T14:35:54.000Z","updated":"2018-04-12T15:08:49.644Z","comments":true,"path":"2018/04/12/markdown文档中mathjax的问题/","link":"","permalink":"http://github.com/2018/04/12/markdown文档中mathjax的问题/","excerpt":"在写markdown文档时经常会需要插入数学公式，我之前只会使用图片插入，上次在看到mathjax后，我开始了使用mathjax的历程，但在实际写文档的过程中遇到了一些问题。","text":"在写markdown文档时经常会需要插入数学公式，我之前只会使用图片插入，上次在看到mathjax后，我开始了使用mathjax的历程，但在实际写文档的过程中遇到了一些问题。 关于有一些公式无法正确的显示在写机器学习的文章中遇到的一个关于范数的公式写出来编辑器上显示没有问题，但是一旦放进文档里就不行了，这个问题困扰了我很长时间。 这是代码：12&gt; 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义&gt; $$ \\lim_&#123;k\\to\\infty&#125;\\left( \\sum_&#123;i=1&#125;^n\\mid p_i-q_i\\mid ^k\\right)^\\frac&#123;1&#125;&#123;k&#125; $$ 这是效果： 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义$$ \\lim{k\\to\\infty}\\left( \\sum{i=1}^n\\mid p_i-q_i\\mid ^k\\right)^\\frac{1}{k} $$ 这里haroopad显示的公式是正确的，但是hexo编译过后的网页显示就不对了。 把代码剪裁一下，看看什么样子的公式是可以的：12&gt; 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义&gt; $$ \\lim_&#123;k\\to\\infty&#125;\\left( \\sum_i \\right) $$ 效果： 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义$$ \\lim_{k\\to\\infty}\\left( \\sum_i \\right) $$ 这个好像就可以，但是貌似sum后面的i一旦加上花括号就不行：12&gt; 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义&gt; $$ \\lim_&#123;k\\to\\infty&#125;\\left( \\sum_&#123;i&#125; \\right) $$ 效果： 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义$$ \\lim{k\\to\\infty}\\left( \\sum{i} \\right) $$ 于是我点开了两个网页的源代码，定位到这一行：1&lt;p&gt;严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义&lt;br&gt;$$ \\lim&lt;em&gt;&#123;k\\to\\infty&#125;\\left( \\sum&lt;/em&gt;&#123;i&#125; \\right) $$&lt;/p&gt; 1&lt;p&gt;严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义&lt;br&gt;$$ \\lim_&#123;k\\to\\infty&#125;\\left( \\sum_i \\right) $$&lt;/p&gt; 可以发现最明显的不同就算lim后面的 &lt;em&gt;，这时我们注意到，hexo在编译的时候将lim和sum后面的下划线 _翻译成强调的 &lt;em&gt; 了，仔细观察前面的公式，确实可以发现一部分变成了斜体。所以我们要在所有的下划线 _ 前面加上 \\ 转义就可以了。 OK，搞定 p.s 我的chrome上显示的公式后面都有一个竖线，firefox没有，内啥，一般平时用chrome习惯，所以有人知道怎么弄咩？ 上面的问题在重新配置Hexo之后就没有了，个人觉得应该是版本的问题？","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://github.com/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://github.com/tags/Markdown/"}]},{"title":"Convolutional Neural Networks / Week 3","slug":"Convolutional-Neural-Networks-Week-3","date":"2018-04-12T09:09:44.000Z","updated":"2018-04-12T14:57:16.915Z","comments":true,"path":"2018/04/12/Convolutional-Neural-Networks-Week-3/","link":"","permalink":"http://github.com/2018/04/12/Convolutional-Neural-Networks-Week-3/","excerpt":"Object LocalizationObject localization用来识别图像中是否包含特定对象以及该对象的位置，并最终使用一个矩形框在图像中标出该特定对象。为了简化问题，在这里我们假设图片中最多包含一个待识别的对象。下面对问题进行形式化描述。 定义目标变量$y$ (同时也是神经网络的输出层)，$$y = [P_c~b_x~b_y~b_h~b_w~C_1~C_2~C_3]^T\\tag{1}$$","text":"Object LocalizationObject localization用来识别图像中是否包含特定对象以及该对象的位置，并最终使用一个矩形框在图像中标出该特定对象。为了简化问题，在这里我们假设图片中最多包含一个待识别的对象。下面对问题进行形式化描述。 定义目标变量$y$ (同时也是神经网络的输出层)，$$y = [P_c~b_x~b_y~b_h~b_w~C_1~C_2~C_3]^T\\tag{1}$$ 其中，$P_c$表示图像中是否包含特定对象，$(b_x, b_y)$表示特定对象的中心位置在图像中的坐标（图像左上角坐标为$(0,0)$，右下角坐标为$(1,1)$），$b_h,b_w$分别表示特定对象的高度和宽度，$C_1-C_3$表示特定对象的类型（行人，汽车，摩托车）。 定义损失函数$\\mathcal{L(\\hat{y}, y)}$，$$\\mathcal{L}(\\hat{y}, y) =\\begin{split}\\begin{cases}\\sum_{i=1}^{i=8} (\\hat{y}_i - y_i)^2,&amp;~if~y_1=1 \\\\(\\hat{y}_1 - y_1)^2,&amp;~if~y_1=0\\end{cases}\\end{split}$$这里针对不同的维度都使用了平方差损失函数，可以针对不同的维度使用不同的损失函数。 Landmark Detection有时我们需要识别图中的一些关键点的坐标，这些坐标称为Landmarks。这时候，我们可以定义如下的目标变量$y$$$y = [P~l_{x1}~l_{y1}~\\dots~l_{xn}~l_{yn}]^T$$以识别人面部眼角嘴角为例，其中$P$代表是否包含人脸，$(l_{xi}, l_{yi})$代表关键点的坐标。 Object Detection Object Detction的其中一种办法叫做Sliding windows detection，采用不同尺寸的矩形框，从左至右、从上到下遍历枚举图像的子图，判断子图中是否包含需要的目标对象。很明显，这种办法比较笨，需要消耗大量的计算量。 Convolutional Implementation of Sliding Windows全连接是可以通过卷积来实现的，并且两者直接是等价的。例如，如果$5\\times 5 \\times 16$的卷积层之后接的是一个$400$个神经元的全连接层，那么它等价于$5 \\times 5 \\times 16$的卷积层之后采用400个$5 \\times 5 \\times 16$的filters得到的$1 \\times 1 \\times 400$的卷积层。 得益于卷积的存在，Sliding windows detection可以做到同时预测同一张图像不同子图中是否包含特定对象。但是这样做的不利条件是预测出来的对象的边界（bounding box）会相对不准确。因为这种办法采用的是子图的边界来作为待预测对象的边界。 Bounding Box Predictions这里介绍了YOLO algorithm (You Only Look Once)，该算法用来识别同一张图像上的多个目标简单。它将图像切分为了$M \\times N$的网格并在此基础上构造了卷积神经网络。该网络的输入依然为整张图片，切分并不影响输入，而是决定了网络的输出尺寸为$M \\times N \\times 8$。这样，每个子图就拥有了一个$1 \\times 1 \\times 8$的预测结果，用来表示图像中是否包含特定的对象，如果包含的话，该特定对象的中心位置、长宽以及类别分别是什么。 该算法利用了卷积操作提高了对同一张图像上不同子图的模型训练预测的效率，使得一次训练就可以完成对多个子图的建模（这里有个假设，就是每个子图上只包含最多一个特定对象）。 Intersection Over Union$$Intersection over Union (loU) = \\frac{size~of~intersection}{size~of~union}$$ 通过loU，我们可以知道两个矩形在大小和位置上的相像程度。这样，我们就可以用它来评价object detection算法的优劣。 Non-max Suppression有时候，我们的算法会将相同的对象识别多次，non-max suppression算法用来解决这个问题。举例， 假设卷积神经网络最后的输出为$19 \\times 19 \\times 5$，也就是说图像被切分为了$19 \\times 19$的子图，每个子图的预测结果为一个5维的向量，该向量如下，$$y = [p_c~b_x~b_y~b_h~b_w]^T$$那么，在训练结束之后，non-max suppression算法会执行如下步骤， 扔掉所有$p_c \\le 0.6$的bounding boxes 取出剩余bounding boxes中$p_c$最大的那个bounding box，作为新检测到的目标 删除剩余所有与该box的loU值$\\ge 0.5$的bounding boxes 重复(2-3)步，直到没有bounding boxes剩余 从上可以看出，non-max suppression其实是个简单的贪心算法。 Anchor Boxes在Object detection问题中，还有一个难点就是图像划分出网格后，每个网格中只能最多识别一个对象。为了让单个网格识别多个对象，可以采用Anchor boxes方法。 Anchor boxes方法的思想很简单，将式(1)改为如下形式，$$y = [P_c~b_x~b_y~b_h~b_w~C_1~C_2~C_3~P_c~b_x~b_y~b_h~b_w~C_1~C_2~C_3]^T\\tag{2}$$式(2)表示在识别的过程中采用了两个Anchor box。每个Anchor box都负责识别所有类别的对象。","categories":[{"name":"deep-learning","slug":"deep-learning","permalink":"http://github.com/categories/deep-learning/"}],"tags":[]},{"title":"Convolutional Neural Networks / Week 2","slug":"Convolutional-Neural-Networks-Week-2","date":"2018-04-12T09:09:44.000Z","updated":"2018-04-12T13:57:00.925Z","comments":true,"path":"2018/04/12/Convolutional-Neural-Networks-Week-2/","link":"","permalink":"http://github.com/2018/04/12/Convolutional-Neural-Networks-Week-2/","excerpt":"Classic NetworksLeNet-5的网络结构如下，","text":"Classic NetworksLeNet-5的网络结构如下， AlexNet的网络结构如下， VGG-16的网络结构如下， ResNets在一般的神经网络中，两层神经网络的数学表达如下，$$\\begin{split}&amp; z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]} \\\\&amp; a^{[l+1]} = g(z^{[l+1]}) \\\\&amp; z^{[l+2]} = W^{[l+2]}a^{[l + 1]} + b^{[l+2]} \\\\&amp; a^{[l+2]} = g(z^{[l+2]})\\end{split}$$而在ResNets中，修改了$a^{[l+2]}$的生成方式，变成了，$$a^{[l+2]} = g(z^{[l + 2]} + a^{[l]})\\tag{1}$$这样的两层神经网络称为Residual block，将这样的Residual block串联起来就构成了ResNets。 ResNets解决了传统神经网络中存在的层数不能过深的问题。在传统的神经网络中，随着深度的增加，训练误差会先降后升，而对于ResNets，随着网络层数达到一百甚至一千层，训练误差也可以平缓的下降（也可能出现收敛的现象）。 Why ResNets Work首先来解释一下为什么在传统的网络后面加一个Residual block不会降低原有网络的性能，$$\\begin{split}a^{[l+2]} &amp;= g(z^{[l + 2]} + a^{[l]}) \\\\ &amp;= g((w^{[l+2]}a^{[l + 1]} + b^{[l + 2]}) + a^{[l]})\\end{split}$$假设我们采用的激活函数为RELU，同时$w^{[l+2]}$和$b^{[l+2]}$为0，那么，$$a^{[l+2]} = g(a^{[l]}) = a^{[l]}$$所以由于Residual block的存在，网络在第$l+2$层的时候，很容易退回到$l$层去。这样可以达到一个效果，在最差情况下，后边加上的Residual block仿佛不存在一样，这样就不会影响原先的效果。 Residual block中还有一点值得注意，对于式(1)来说，$z^{[l+2]}$和$a^{[l]}$的维度需要一致，那如果出现维度不一致的情况怎么办呢？增加一个$W_s$矩阵，$$a^{[l+2]} = g(z^{[l + 2]} + W_s a^{[l]})$$$W_s$有两种方式生成， 随机生成的参数矩阵，跟随其他参数一起训练学习。 $a^{[l]}$的基础上采用padding操作生成，比如补0。 Networks in Networks and 1x1 Convolutions1x1 Convolutons也称为Networks in networks，它是一个1x1的filters并使用了Relu非线性变换。 Inception Network Motivation在构造神经网络的时候，我们有时候会很困惑，用$1\\times1$的卷积效果好，还是$f^{[l]}\\times f^{[l]}$的卷积效果好，或者用Max-pooling效果会更好呢？Inception Network的思想是，那就把他们在同一层中都用一遍，这样就会得到若干tensor的输出，然后再把这些tensor在channel的纬度上拼在一起，组合一个大的tensor。最后，用数据去训练学习，决定这些filters的参数。 这样会有如下问题， 如何让这些tensor在除了channel之外的其他维度上保持尺寸一致？ 会不会造成计算量的显著增加？ 对于问题(1)其实很好解决，采用padding的方式就可以让这些tensor的$n_H$和$n_W$保持一致。 对于问题(2)可以通过在两层网络中间增加一层$1\\times1$ filter来解决。下面详细描述原理。 假设我们有这样两层网络， 那么从左到右需要的乘法运算的数量为，$$(28 \\times 28 \\times 32) \\times (5 \\times 5 \\times 192) \\approx 120~million$$如果我们在图(1)两层网络中间加入一层使用了$1\\times1$ filter的卷积层，如图(2)所示， 那么从左到右所需要的乘法运算的数量为，$$\\begin{split}&amp;1st~layer \\rightarrow 2nd~layer：&amp;(28 \\times 28 \\times 16) \\times (1 \\times 1 \\times 192) \\approx 2.4~million \\\\&amp;2nd~layer \\rightarrow 3rd~layer：&amp;(28 \\times 28 \\times 32) \\times (5 \\times 5 \\times 16) \\approx 10~million\\end{split}$$也就是共需要$12.4~million$的乘法运算。从中可以看到，加入”bottleneck layer”之后，所需要的计算量减少为了原来的十分之一。 Transfer Learning当我们有一个比较小的训练数据集的时候，我们可以在别人训练好的模型的基础上来达到我们的目的：删除最后的softmax layers，保留之前的layers的模型结构和权重，并在之后增加我们自己的softmax layers。然后用较小的训练数据集来训练我们新增加的layers的参数。这样就可以用较少的数据来得到不错的预测效果。 其中，我们可以预先存储训练数据集中的样本经过之前的layers (删除原先的softmax layers)之后得到的activations，这样就不用在之后训练新的softmax layers参数的时候反复计算，从而节省计算量并提高效率。 随着我们拥有的训练数据的增加，我们可以保留较少层数的参数不发生改变，其余网络层以原先权重为初始参数，然后在新的训练数据集上进行训练调整。如果我们的训练数据足够大，那么原先所有层的参数都可以只作为初始参数，让它们在新的数据集上进行训练调整。 Data Augmentation通常在机器学习中，我们需要大量的训练数据，因此有一些常用的有效的增加数据集的方法， Mirroring: 镜像处理 Random Cropping: 随机图像裁剪 Rotation: 图像旋转 Shearing: Local warping: 局部变形 Color Shifting: 在不同的颜色通道上增减一定的数值，例如$R+20, G-20, B+20$","categories":[{"name":"deep-learning","slug":"deep-learning","permalink":"http://github.com/categories/deep-learning/"}],"tags":[]},{"title":"Convolutional Neural Networks / Week 4","slug":"Convolutional-Neural-Networks-Week-4","date":"2018-04-12T09:09:44.000Z","updated":"2018-04-12T15:06:09.549Z","comments":true,"path":"2018/04/12/Convolutional-Neural-Networks-Week-4/","link":"","permalink":"http://github.com/2018/04/12/Convolutional-Neural-Networks-Week-4/","excerpt":"What is Face Recognition这里区分两个概念，Face verification和Face recognition， Verification 输入：图像 + 名字/ID 输出：输入的图像上是否有名字/ID表示的那个人 Recognition 数据库中有$K$个人 输入：图像 输出：图像上有的数据库中的人的名字/ID 很明显，Recognition比Verification的难度要大得多。","text":"What is Face Recognition这里区分两个概念，Face verification和Face recognition， Verification 输入：图像 + 名字/ID 输出：输入的图像上是否有名字/ID表示的那个人 Recognition 数据库中有$K$个人 输入：图像 输出：图像上有的数据库中的人的名字/ID 很明显，Recognition比Verification的难度要大得多。 One Shot Learning什么是One-shot learning， Learning from one example to recognize the person again. 很多时候公司的数据库中只有一张员工的照片，那么我们在做人脸识别系统的时候怎么根据这一张照片来再次识别相同的人的影像？如果用传统的卷积神经网络来做的话，因为训练数据很小，所以通常效果并不理想。 在这种情况下，我们应该学习”similarity” function,$$d(img1, img2) = degree~of~difference~between~images$$然后可以根据Similarity function完成verification,$$\\begin{split}If~d(img1, img2) &amp;\\le \\tau~same\\\\&amp;&gt; \\tau~different\\end{split}$$ Siamese NetworkParameters of NN define an encoding $f(x^{(i)})$ Learn parameters so that: If $x^{(i)}, x^{(j)}$ are the same person, $| f(x^{(i)}) - f(x^{(j)}) |^2$ is small If $x^{(i)}, x^{(j)}$ are the different person, $| f(x^{(i)}) - f(x^{(j)}) |^2$ is large 那么，该网络学习的目标函数应该怎么定义呢？ Triple Loss在Triple loss中，基准人脸图像称为Anchor image，正样本为Positive image，负样本为Negative image，那么我们希望得到的是，$$|f(A) - f(P) | ^2 \\le | f(A) - f(N) | ^2$$也就是，$$|f(A) - f(P) | ^2 - | f(A) - f(N) | ^2 \\le 0$$这里有个问题，如果$f$始终预测0，那么上述条件始终满足。为了防止这种情况发生，所以需要增加一个超参$\\alpha$，也称为margin，$$|f(A) - f(P) | ^2 - | f(A) - f(N) | ^2 + \\alpha \\le 0$$下面对Loss function进行形式化定义，给定3个图像 A, P, N，$$\\mathcal{L}(A, P, N) = max(|f(A) - f(P)|^2 -|f(A) - f(N)|^2 + \\alpha, 0)$$在整体样本上的损失为，$$J = \\sum_{i=1}^{M} \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})$$很明显，在训练样本中，一个人需要有多张照片才能组合出这样的三元组。 那么，应该怎么选择三元组呢， During training, if A, P, N are chsen randomly, $d(A, P) + \\alpha \\le d(A, N)$ is easily satisfied. 这种方法容易选择，但是训练出来的模型效果一般 Choose triplets that’re “hard” to train on. 这种方法不容易选择，但是模型能学习到更多的信息。 Face Verification and Binary Classification另一种similarity function，以一对图像作为输入，$$\\hat{y} = \\delta ( \\sum_{k=1}^{128} w_i | f(x^{(i)})_k - f(x^{(j)})_k | + b )$$其中，$f(x^{(i)})$表示对第i张图像的128维的embedding表达。 What are deep ConvNets Learning这里介绍了怎么将卷积神经网络的hidden layer可视化。以第一层为例， Pick a unit in layer 1. Find the nine image patches that maximize the unit’s activation. Repeat for other units. 可以看到，每个unit’s activation针对的方向不同，有的是颜色，有的是不同方向的边。随着网络深度的增加，每个unit’s activation可以看到的图像的范围越来越大。 Neural Style Transfer Cost Function原始内容图片为C，风格图片为S，目标图片为G，那么，$G_{kk’}^{[l]}$$$J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G)$$ 下面介绍Style matrix,$$\\begin{split}&amp; Let~a_{i,j,k}^{[l]} &amp;= activation~at~(i,j,k). G^{[l]}~is~n_c^{[l]} \\times n_c^{[l]} \\\\\\rightarrow &amp; G_{kk’}^{[l]} &amp;= \\sum_{i=1}^{n_H^{[l]}} \\sum_{j=1}^{[n_W]^{[l]}} a_{ijk}^{[l]} a_{ijk’}^{[l]}\\end{split}$$其中，$G^{[l]}$为Style matrix。","categories":[{"name":"deep-learning","slug":"deep-learning","permalink":"http://github.com/categories/deep-learning/"}],"tags":[]},{"title":"Convolutional Neural Networks / Week 1","slug":"Convolutional-Neural-Networks-Week-1","date":"2018-04-12T09:09:44.000Z","updated":"2018-04-12T12:42:44.005Z","comments":true,"path":"2018/04/12/Convolutional-Neural-Networks-Week-1/","link":"","permalink":"http://github.com/2018/04/12/Convolutional-Neural-Networks-Week-1/","excerpt":"Computer VisionComputer Vision Problems include: Image Classication Object Detection … … One of the challenges of computer vision problems is that the input can be very big. For example, a 1000 by 1000 image can have $1000 \\times 64 \\times 3 = 12288$ dimensions because there are three color channels. If the size of hidden layer is 1000, the number of parameters from input layer to hidden layer could be 3 billion. This will cause these problems: data size requirements; computational requirements; memory requirements.","text":"Computer VisionComputer Vision Problems include: Image Classication Object Detection … … One of the challenges of computer vision problems is that the input can be very big. For example, a 1000 by 1000 image can have $1000 \\times 64 \\times 3 = 12288$ dimensions because there are three color channels. If the size of hidden layer is 1000, the number of parameters from input layer to hidden layer could be 3 billion. This will cause these problems: data size requirements; computational requirements; memory requirements. PaddingThe problems of convolutional operation: shrinking output throwing away a lot of information from the edges of the image In order to fix these problems, what we need to do is pad the image. 通常有两种padding的方式： Valid convolution: 意思是不采用padding的方式。 Same convolution：意思是输出的尺寸和输入的尺寸相同。 在这种情况下可以推导出$p = \\frac{f-1}{2}$，所以在计算机视觉的模型中，filter的尺寸通常是奇数而不是偶数。 Filter的尺寸是奇数还有另外一个好处，就是filter可以有中心像素，可以很方便的用来定位filter的位置。 Strided Convolutions给定如下条件，$$\\begin{split}&amp;n\\times n~image~~~~&amp;f\\times f~filter\\\\&amp;padding~p&amp;stride~s\\end{split}$$经过Strided Convolutions之后得到的tensor的尺寸为，$$\\lfloor \\frac{n + 2p - f}{s} + 1 \\rfloor \\times \\lfloor \\frac{n + 2p - f}{s} + 1 \\rfloor$$ NG在这里提到，我们所谓的convolution并不是真正意义上的卷积，而是应该称为cross-correlation，它之前实际上应该有一个针对卷积核的变换操作，这些操作再加上cross-correlation才是真正的convolution。但是这个变换操作没什么用处，所以通常情况下就省略了。 Convolutions Over Volume在RGB类型的多通道图像中使用Multiple filters：$$n \\times n \\times n_c \\ast f\\times f \\times n_c \\rightarrow (n - f + 1) \\times (n - f + 1) \\times {n_c}’$$其中，$n$表示图像的长宽，$f$表示filter的长宽，$n_c$表示图像的通道数，$n_c’$表示filter的个数。 One Layer of a Convolutional Network普通的BP神经网络的数学表达形式如下：$$\\begin{split}z^{[1]} &amp;= w^{[1]} a^{[0]} + b^{[1]} \\\\a^{[1]} &amp;= g(z^{[1]})\\end{split}$$在CNN中，convolution operation相当于$w^{[1]}a^{[0]}$，也就是充当了原先线性变换的角色。 这里对卷积层中涉及到的符号进行总结，$$\\begin{split}f^{[l]} &amp;= filter~size \\\\p^{[l]} &amp;= padding \\\\s^{[l]} &amp;= stride \\\\n_{C}^{[l]} &amp;= number~of~filters\\end{split}\\tag{1}$$接着定义卷积层的输入和输出表示，$$\\begin{split}Input:~&amp;n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_C^{[l-1]} \\\\Output:~&amp;n_H^{[l]} \\times n_W^{[l]} \\times n_C^{[l]}\\end{split}\\tag{2}$$ 基于(1)和(2)，我们可以进行如下定义，$$\\begin{split}&amp;Each~filter~is:~ &amp;f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]} \\\\&amp;Activations:~ &amp;a^{[l]} \\rightarrow n_H^{[l]} \\times n_W^{[l]} \\times n_C^{[l]} \\\\&amp;Weights:~ &amp;f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]} \\times n_C^{[l]} \\\\&amp;bias:~ &amp;n_C^{[l]}\\end{split}\\tag{3}$$在式(2)中，$n_H^{[l]}$与$n_H^{[l-1]}$的关系如下，$$n_H^{[l]} = \\lfloor \\frac{n_H^{[l - 1]} + 2p^{[l]} - f^{[l]}}{s^{l}} + 1 \\rfloor$$在式(3)中，Activations是单个样本的形式，batch的形式如下，$$A^{[l]} \\rightarrow m \\times n_H^{[l]} \\times n_W^{[l]} \\times n_C^{[l]}$$ Simple Convolution Network Example以图像分类为例（识别图片中是否有猫），经过若干卷积层之后，为了得到最终的$0/1$分类结果，会将最后一层卷积的tensor展开并拉长成vector，经过logistic/softmax单元后得到代表预测结果的概率值。 在使用ConvNet的过程中，比较麻烦的地方在于如何确定超参。有一个常用的指导方针是，activations的长和宽需要越来越小（也就是图片的尺寸越来越小），同时通道数需要越来越多（也就是activations的第三个维度）。之后会详细介绍怎么需选择超参。 在ConvNet中，通常有三种类型的网络层， Convolution (CONV) Pooling (POOL) Fully connected (FC) Pooling LayersPooling Layer有如下好处， 减少图像representation的尺寸，提高计算速度 提高鲁棒性 很有意思的地方在于，对于pooling layer来说，我们只需要确定超参数$f^{[l]}$和$s^{[l]}$，以及是max pooling 还是average pooling，并不需要进行参数的学习。 在pooling layer中，超参$p^{[l]}$通常设置为0。 CNN Example神经网络中常用的一种模式是，若干卷积层之后加池化层，再若干层卷积层之后接池化层，然后接全连接层，最后给softmax单元。NG在课上画了一个例子如下， Why Convolutions卷积层最显著的特点就是参数数量大大小于全连接层的数量，因为： Parameter sharing: 图像的不同位置共享filters。 Sparsity of connections: 每个输出值只取决于很小的一部分输入。这样也降低了过拟合的风险。 卷积神经网络的损失函数定义如下所示，$$Cost~J = \\frac{1}{m} \\sum_{i-1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$$ Programming AssignmentsConvolutional Model: step by step卷积层和池化层的区别： 卷积层中的每个filter都会同时作用在不同的channel上 在池化层中，filter与channel一一对应，作用在对应的channel上 References MarkDown中使用Latex数学公式 Hexo博客(13)添加MathJax数学公式渲染 解释了Markdown和Mathjax渲染冲突问题","categories":[{"name":"deep-learning","slug":"deep-learning","permalink":"http://github.com/categories/deep-learning/"}],"tags":[]},{"title":"利用GitHub搭建一个你的博客","slug":"利用GitHub搭建一个你的博客","date":"2018-04-12T08:04:26.000Z","updated":"2018-04-12T08:14:41.109Z","comments":true,"path":"2018/04/12/利用GitHub搭建一个你的博客/","link":"","permalink":"http://github.com/2018/04/12/利用GitHub搭建一个你的博客/","excerpt":"为什么要写博客作为一只程序猿，踩到坑是一件非常正常的事，当我们踩到坑的时候就会花心思去研究它，可能我们能够在当时把问题弄懂并把问题给解决掉。可是过一段时间我们又遇到了同样的坑的时候，难道还要再去 百毒 Google 重新搜索一遍吗？这样做效率难免太低了，倒不如在第一次解决问题的时候就把解决方法写到我们的博客了，当我们再一次遇到相同的坑的时候翻一翻我们之前写的博客就能快速的把问题给解决掉，何乐而不为。而且我们学习新技术的时候也可以将当时学到的内容写到我们的博客，再次遇到的时候我们就可以找回当时学习的思路，继续学习。废话不多说，马上开始行动起来，搭建博客！ 声明：本文在Windows下进行操作的，Mac以及其它操作系统请做参考 多图警告！","text":"为什么要写博客作为一只程序猿，踩到坑是一件非常正常的事，当我们踩到坑的时候就会花心思去研究它，可能我们能够在当时把问题弄懂并把问题给解决掉。可是过一段时间我们又遇到了同样的坑的时候，难道还要再去 百毒 Google 重新搜索一遍吗？这样做效率难免太低了，倒不如在第一次解决问题的时候就把解决方法写到我们的博客了，当我们再一次遇到相同的坑的时候翻一翻我们之前写的博客就能快速的把问题给解决掉，何乐而不为。而且我们学习新技术的时候也可以将当时学到的内容写到我们的博客，再次遇到的时候我们就可以找回当时学习的思路，继续学习。废话不多说，马上开始行动起来，搭建博客！ 声明：本文在Windows下进行操作的，Mac以及其它操作系统请做参考 多图警告！ 1.环境搭建首先需要下载两个东西 node.js git 具体的下载，安装就不用多说了，基本上下载完默认安装即可，安装的路径最好先记住。Git 安装的时候会弹出下面的窗口，我们选择第二个即可。这样我们在Windows的命令窗口也可以进行Git操作了。两个都安装完了之后，打开命令窗口（按住Win+R后输入CMD即可打开命令窗口），分别输入 node -v 、npm -v 及 git –version 这三个命令是为了查看刚才我们安装的软件的版本，如果你能够看到他们的版本号（如同下图，也许版本号会有不同），那么恭喜你，环境搭建这一个大难关你已经过了，可以进入下一步骤了。 1.1有问题看这里 打开我们刚才安装软件的路径，例如我的路径“D:\\Program Files\\nodejs”、“D:\\Program Files\\Git”。复制我们刚才安装的路径,打开计算机&gt;右键单击属性，选择高级系统设置&gt;选择环境变量&gt;双击 PATH &gt;将我们安装的路径追加到变量值之后 ！注意分号以及确定保存这个时候再试一下 node -v 、npm -v 及 git –version 这三个命令，一般都不会有问题的了。 2.配置 GitHub2.1注册 GitHub先到GitHub官网Sign up(注册)一个账号。填好用户名、邮箱、密码进入下一步 2.2SSH授权注册好账号之后我们可以随意的查看其他人的项目，甚至是clone下载，但是要提交代码就必须完成SSH授权，如果可以不用授权就提交代码的话，那么Github岂不是乱了套。 2.2.1生成SSH key打开Git Bash，输入ssh-keygen -t rsa然后按三下回车，如下图所示接着就会在C:\\Users\\Administrator.ssh目录下生成到id_rsa和id_rsa.pub两个文件，id_rsa是密钥，id_rsa.pub是公钥，接下来需要将id_rsa.pub的内容添加到GitHub上，这样本地的id_rsa密钥才能跟GitHub上的id_rsa.pub公钥进行配对，才能够授权成功。 2.2.2在GitHub上添加SSH Key首先点击右上角的倒三角进入Settings紧接着选择左侧SSH and GPG keys,然后选择右上角的New SSH key，再把id_sra.pub的内容复制粘贴到key（id_sra.pub可以使用 sublime 或者 记事本打开），最后Add SSH key就可以了。SSH key 添加成功之后，输入 ssh -T git@github.com 进行测试，如果出现以下提示证明添加成功了。 这一步一般没什么问题，有问题的话留言评论（顺便来一波打赏）就好了。直接进入下一步！ 3.创建 GitHub 仓库 需要特别注意的是，项目名称一定要使用 你的名字 + .github.io这一步也没什么问题，如果有问题，一定是你没有给我打赏(∩_∩) 4.设置本地博客的配置4.1安装Hexo在你认为合适的地方创建一个文件夹，然后在文件夹空白处按住 Shift+鼠标右键，然后点击在此处打开命令行窗口。（同样要记住啦，下文中会使用在当前目录打开命令行来代指上述的操作）在命令行输入npm install -g hexo然后输入 npm install hexo –save 这时候你会看到命令窗口刷了一堆白字，然后输入 hexo -v 查看hexo是否安装成功了。如果出现与上图一样的情况的话，就说明你离成功就近在咫尺了。 4.2初始化Hexo同样是在命令窗口中，继续输入 hexo init，等待下载好了之后输入 hexo s这时候我们就可以打开浏览器了，在地址栏中输入 http://localhost:4000/ 我们就可以看到如下图的界面，这个就是我们的博客。没错，我们的博客就这样建好了。不过这个只是我们本地的博客，下面就要考虑怎么把我们的本地博客上传到我们的GitHub上了。接下来先看一下我们的博客文章放在哪里。打开我们的文件夹下面的source文件夹，你会发现里面有一个_posts文件夹，再进入就会看到一片初始化的文章hello-world.md也就是上图显示在页面的文章。如果我们想新建文章的话，可以通过命令窗口输入hexo new ‘filename’我们的文件夹下面就会生成一个新的md文件，然后我们打开编辑就可以了。 4.3发布博客首先复制我们的GitHub项目地址，如下图。然后打开我们新建的文件夹下面生成的_config.yml文件，在最下方作如下修改。deploy 是部署的意思，type: git 就是使用 git 进行部署，repo: github仓库地址 注意：repo 原本是没有的，在最后自己加上就好。冒号之后有一个空格 冒号之后有一个空格 冒号之后有一个空格 接下来回到命令窗口，输入 npm install hexo-deployer-git –save安装好Git上传插件之后，输入 hexo g，然后输入 hexo d就可以将我们的博客上传到我们的GitHub了，而且以后更新文章就只需要用这两个命令就可以了。这样别人也可以通过 https://yourname.github.io 来访问我们的博客了（开头一定要用https，yourname是你的github的名字）。 5.个性化设置（更换主题）有木有觉得这个博客的默认主题特别的丑，如果不觉得可以忽略这一步（哈哈）。这里以我使用的主题为例。第一步去找我们想要的主题，然后下载下来。我用的是next主题，在命令窗口输入git clone https://github.com/iissnan/hexo-theme-next themes/next然后打开配置文件，找到 theme 将原来默认的 landscape 替换 next。然后在命令窗口输入 hexo clean 、hexo g 及 hexo s，先看一下本地博客是什么样子，确认好了在输入 hexo d 部署到GitHub每一个主题都有一个使用文档，next的使用文档为 http://theme-next.iissnan.com/getting-started.html 我们可以为我们的主题修改名字，添加评论等等，具体的你们就自己去研究了。 如果文章对你有所帮助，那么请您点一下❤由于本人水平有限，如有错误，欢迎大家指正。如果你在操作过程中发现一些没有讲到的错误或者问题，欢迎在评论留言，一起探讨，共同学习进步！有钱的来波赞赏，没钱的来波Star","categories":[{"name":"blog","slug":"blog","permalink":"http://github.com/categories/blog/"}],"tags":[{"name":"Git笔记","slug":"Git笔记","permalink":"http://github.com/tags/Git笔记/"}]},{"title":"Git--命令(一)","slug":"Git--命令(一)","date":"2018-04-12T07:02:42.000Z","updated":"2018-04-13T14:02:27.347Z","comments":true,"path":"2018/04/12/Git--命令(一)/","link":"","permalink":"http://github.com/2018/04/12/Git--命令(一)/","excerpt":"git命令学习 Git笔记 GitHub介绍GitHub 是为开发者提供 Git 仓库的托管服务。这是一个让开发者与朋友、同事、同学及陌生人共享代码的完美场所。总结一下，GitHub 最大的特征是“面向人” Git是一个“分布式版本管理工具”，简单的理解版本管理工具：大家在写东西的时候都用过“回撤”这个功能，但是回撤只能回撤几步，假如想要找回我三天之前的修改，光用“回撤”是找不回来的。而“版本管理工具”能记录每次的修改，只要提交到版本仓库，你就可以找到之前任何时刻的状态（文本状态）。 统一概念 工作区：改动（增删文件和内容） 暂存区：输入命令：git add 改动的文件名，此次改动就放到了‘暂存区’ 本地仓库(简称：本地)：输入命令：git commit 此次修改的描述，此次改动就放到了’本地仓库’，每个commit，我叫它为一个‘版本’。 远程仓库(简称：远程)：输入命令：git push 远程仓库，此次改动就放到了‘远程仓库’（GitHub等) 分支：Git鼓励你使用branch完成某个任务，合并后再删掉分支，过程更安全。 commit-id：输出命令：git log，最上面那行commit xxxxxx，后面的字符串就是commit-id 参考tips项目,和廖雪峰老师的git网站。","text":"git命令学习 Git笔记 GitHub介绍GitHub 是为开发者提供 Git 仓库的托管服务。这是一个让开发者与朋友、同事、同学及陌生人共享代码的完美场所。总结一下，GitHub 最大的特征是“面向人” Git是一个“分布式版本管理工具”，简单的理解版本管理工具：大家在写东西的时候都用过“回撤”这个功能，但是回撤只能回撤几步，假如想要找回我三天之前的修改，光用“回撤”是找不回来的。而“版本管理工具”能记录每次的修改，只要提交到版本仓库，你就可以找到之前任何时刻的状态（文本状态）。 统一概念 工作区：改动（增删文件和内容） 暂存区：输入命令：git add 改动的文件名，此次改动就放到了‘暂存区’ 本地仓库(简称：本地)：输入命令：git commit 此次修改的描述，此次改动就放到了’本地仓库’，每个commit，我叫它为一个‘版本’。 远程仓库(简称：远程)：输入命令：git push 远程仓库，此次改动就放到了‘远程仓库’（GitHub等) 分支：Git鼓励你使用branch完成某个任务，合并后再删掉分支，过程更安全。 commit-id：输出命令：git log，最上面那行commit xxxxxx，后面的字符串就是commit-id 参考tips项目,和廖雪峰老师的git网站。 Git介绍 Git是分布式版本控制系统 集中式VS分布式，SVN VS Git SVN和Git主要的区别在于历史版本维护的位置 Git本地仓库包含代码库还有历史库，在本地的环境开发就可以记录历史而SVN的历史库存在于中央仓库，每次对比与提交代码都必须连接到中央仓库才能进行。 这样的好处在于： 自己可以在脱机环境查看开发的版本历史。 多人开发时如果充当中央仓库的Git仓库挂了，可以随时创建一个新的中央仓库然后同步就立刻恢复了中央库。 Git命令Git配置12$ git config --global user.name \"Your Name\"$ git config --global user.email \"email@example.com\" git config命令的--global参数，表明这台机器上的所有Git仓库都会使用这个配置，也可以对某个仓库指定不同的用户名和邮箱地址。 创建版本库初始化一个Git仓库1$ git init 添加文件到Git仓库包括两步：12$ git add &lt;file&gt;$ git commit -m \"description\" 12345678#把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件。$ git add . #仅监控已经被add的文件,不会提交新文件（untracked file）$ git add -u#是上面两个功能的合集（git add --all的缩写）$ git add -A git add可以反复多次使用，添加多个文件，git commit可以一次提交很多文件，-m后面输入的是本次提交的说明，可以输入任意内容。 显示文件树形结构1234567$ tree /f #指定tree使用字符而不是图形字符显示链接子目录的行$ tree /a #显示每个目录中的文件名``### 查看工作区状态```bash$ git status 查看修改内容1$ git diff 1$ git diff --cached 1$ git diff HEAD -- &lt;file&gt; git diff 可以查看工作区(work dict)和暂存区(stage)的区别 git diff --cached 可以查看暂存区(stage)和分支(master)的区别 git diff HEAD -- &lt;file&gt; 可以查看工作区和版本库里面最新版本的区别 查看提交日志1$ git log 简化日志输出信息1$ git log --pretty=oneline 查看命令历史1$ git reflog 版本回退1$ git reset --hard HEAD^ 以上命令是返回上一个版本，在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本是HEAD^^，往上100个版本写成HEAD~100。 回退指定版本号1$ git reset --hard commit_id commit_id是版本号，是一个用SHA1计算出的序列 工作区、暂存区和版本库工作区：在电脑里能看到的目录；版本库：在工作区有一个隐藏目录.git，是Git的版本库。Git的版本库中存了很多东西，其中最重要的就是称为stage（或者称为index）的暂存区，还有Git自动创建的master，以及指向master的指针HEAD。 进一步解释一些命令： git add实际上是把文件添加到暂存区 git commit实际上是把暂存区的所有内容提交到当前分支 撤销修改丢弃工作区的修改1$ git checkout -- &lt;file&gt; 该命令是指将文件在工作区的修改全部撤销，这里有两种情况： 一种是file自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是file已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 丢弃暂存区的修改分两步：第一步，把暂存区的修改撤销掉(unstage)，重新放回工作区：1$ git reset HEAD &lt;file&gt; 第二步，撤销工作区的修改1$ git checkout -- &lt;file&gt; 小结： 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- &lt;file&gt;。 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了第一步，第二步按第一步操作。 已经提交了不合适的修改到版本库时，想要撤销本次提交，进行版本回退，前提是没有推送到远程库。 删除文件1$ git rm &lt;file&gt; git rm &lt;file&gt;相当于执行12$ rm &lt;file&gt;$ git add &lt;file&gt; 进一步的解释Q：比如执行了rm text.txt 误删了怎么恢复？A：执行git checkout -- text.txt 把版本库的东西重新写回工作区就行了Q：如果执行了git rm text.txt我们会发现工作区的text.txt也删除了，怎么恢复？A：先撤销暂存区修改，重新放回工作区，然后再从版本库写回到工作区12$ git reset head text.txt$ git checkout -- text.txt Q：如果真的想从版本库里面删除文件怎么做？A：执行git commit -m &quot;delete text.txt&quot;，提交后最新的版本库将不包含这个文件 远程仓库创建SSH Key1$ ssh-keygen -t rsa -C \"youremail@example.com\" 关联远程仓库1$ git remote add origin https://github.com/username/repositoryname.git 推送到远程仓库1$ git push -u origin master -u 表示第一次推送master分支的所有内容，此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改。 从远程克隆1$ git clone https://github.com/usern/repositoryname.git 分支创建分支1$ git branch &lt;branchname&gt; 查看分支1$ git branch git branch命令会列出所有分支，当前分支前面会标一个*号。 切换分支1$ git checkout &lt;branchname&gt; 创建+切换分支1$ git checkout -b &lt;branchname&gt; 合并某分支到当前分支1$ git merge &lt;branchname&gt; 删除分支1$ git branch -d &lt;branchname&gt; 查看分支合并图1$ git log --graph 当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。用git log --graph命令可以看到分支合并图。 普通模式合并分支1$ git merge --no-ff -m \"description\" &lt;branchname&gt; 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。合并分支时，加上--no-ff参数就可以用普通模式合并，能看出来曾经做过合并，包含作者和时间戳等信息，而fast forward合并就看不出来曾经做过合并。 保存工作现场1$ git stash 查看工作现场1$ git stash list 恢复工作现场1$ git stash pop 丢弃一个没有合并过的分支1$ git branch -D &lt;branchname&gt; 查看远程库信息1$ git remote -v 在本地创建和远程分支对应的分支1$ git checkout -b branch-name origin/branch-name， 本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联1$ git branch --set-upstream branch-name origin/branch-name； 从本地推送分支1$ git push origin branch-name 如果推送失败，先用git pull抓取远程的新提交； 从远程抓取分支1$ git pull 如果有冲突，要先处理冲突。 标签tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 新建一个标签1$ git tag &lt;tagname&gt; 命令git tag &lt;tagname&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id。 指定标签信息1$ git tag -a &lt;tagname&gt; -m &lt;description&gt; &lt;branchname&gt; or commit_id git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息。 PGP签名标签1$ git tag -s &lt;tagname&gt; -m &lt;description&gt; &lt;branchname&gt; or commit_id git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;可以用PGP签名标签。 查看所有标签1$ git tag 推送一个本地标签1$ git push origin &lt;tagname&gt; 推送全部未推送过的本地标签1$ git push origin --tags 删除一个本地标签1$ git tag -d &lt;tagname&gt; 删除一个远程标签1$ git push origin :refs/tags/&lt;tagname&gt;","categories":[{"name":"工具","slug":"工具","permalink":"http://github.com/categories/工具/"}],"tags":[{"name":"git","slug":"git","permalink":"http://github.com/tags/git/"}]},{"title":"Markdown语法","slug":"Markdown语法","date":"2018-04-12T02:39:01.000Z","updated":"2018-04-12T03:29:16.648Z","comments":true,"path":"2018/04/12/Markdown语法/","link":"","permalink":"http://github.com/2018/04/12/Markdown语法/","excerpt":"针对中文,演示Markdown的语法 大标题 大标题一般显示工程名，类似html的h1 你只要在标题下面跟上=====即可（超过3个=即可，长度不限） 或者你可以在标题前面加一个 # 来实现（# 大标题）","text":"针对中文,演示Markdown的语法 大标题 大标题一般显示工程名，类似html的h1 你只要在标题下面跟上=====即可（超过3个=即可，长度不限） 或者你可以在标题前面加一个 # 来实现（# 大标题） 中标题 中标题一般显示重点项，类似html的h2 你只要在标题下面输入—–即可（超过3个-即可，长度不限） 或者你可以在标题前面加两个 # 来实现（## 中标题） 小标题 小标题类似html的h3 你只要在标题前面加三个 # 即可（### 小标题） 注意，下面所有语法的提示我都先用小标题提醒了！ 四级标题五级标题六级标题 四、五、六级标题类似html的h4、h5、h6 由前面类推，你只要在标题前面加四个、五个、六个 # 来实现 水平标尺 你只要在一个空行画上—–即可（超过3个-即可，长度不限） 但注意，前一行不能有纯文字，否者会当作中标题处理！ 文本框这是一个有多行的文本框 常用来在这里写入代码 只要每行文字前面输入一个Tab（或四个空格） 再输入文字，即可实现效果 比如我们可以在多行文本框里输入一段代码,来一个C++版本的HelloWorld吧 #include &lt;iostream&gt; using namespace std; int main(){ cout&lt;&lt;&quot;HelloWorld!&quot;&lt;&lt;endl; return 0; } 代码块 书写示例： 1234// 代码区域的上下分别用三个 ` 括起来public class Person &#123; // 代码缩进请使用 四个空格，不要使用 Tab&#125; 效果： 1234// 代码区域的上下分别用三个 ` 括起来public class Person &#123; // 代码缩进请使用 四个空格，不要使用 Tab&#125; 链接1.点击这里你可以链接到Google2.点击这里我你可以链接到我的GitHub链接插入的格式是：[链接的显示文字](链接URL) 图片图片插入的格式是：![图像替代文本](图片URL “图片说明文字”) 图片链接比如我想点击GitHub的图片，然后再进入GitHub首页图片链接插入的格式是：![[图像替代文本](图片URL “图片说明文字”)](链接URL) 引用 段落前面用竖线来框定要引用的文字只要再文字前面加上&gt; 即可但&gt; 只能放在行首才有效 多重引用 段落前面用竖线来框定要引用的文字 只要再文字前面加上&gt; 即可 但&gt; 只能放在行首才有效 无序列表 在行首加点 行首输入* 再空格，输入内容即可 有序列表 在行首加数字标号 行首输入数字和一个点（2.） 再空格，输入内容即可 二级列表（不带序号）书写示例： 1234- 列表 1（一级列表：减号 + 空格） - 列表 1.1（二级列表：四个空格 + 减号 + 空格） - 列表 1.2- 列表 2 效果： 列表 1（一级列表：减号 + 空格） 列表 1.1（二级列表：四个空格 + 减号 + 空格） 列表 1.2 列表 2 二级列表（带序号）书写示例： 1234- 列表（一级列表：减号 + 空格） 1. 列表（二级列表：四个空格 + 序号 + 点 + 空格） 2. 列表- 列表 2 效果： 列表 1（一级列表：减号 + 空格） 列表（二级列表：四个空格 + 自然数 + 点 + 空格） 列表 列表 2 字体 斜体只需要在要加斜体的文字前后各加上一个*号即可（*要加斜体的文字*） 粗体只需要在要加粗体的文字前后各加上两个*号即可（**要加粗体的文字**） 特殊字符有一些特殊字符如&lt;,#等,只要在特殊字符前面加上转义字符\\即可你想换行的话其实可以直接用html标签\\ 换行书写示例： 12我是第一行（后面有两个空格） 我是第二行 效果： 我是第一行（后面有两个空格）我是第二行 标亮书写示例： 1`请把我标亮` 效果： 请把我标亮 表格书写示例： 1234567|Prefix |Framework ||--------|------------||NS |Foundation (OS X and iOS) and Application Kit (OS X) ||UI |UIKit (iOS) ||AB |Address Book ||CA |Core Animation ||CI |Core Image | 效果： Prefix Framework NS Foundation (OS X and iOS) and Application Kit (OS X) UI UIKit (iOS) AB Address Book CA Core Animation CI Core Image 免费编辑器Windows 平台MarkdownPadhttp://markdownpad.com Mac 平台MacDownhttp://macdown.uranusjr.com/ 注：不推荐 Mou，原因 文件大了超级卡。","categories":[{"name":"工具","slug":"工具","permalink":"http://github.com/categories/工具/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://github.com/tags/Markdown/"}]},{"title":"【machine Learning】机器学习-1","slug":"【machine Learning】机器学习-1","date":"2018-04-11T08:00:08.000Z","updated":"2018-04-12T16:52:41.330Z","comments":true,"path":"2018/04/11/【machine Learning】机器学习-1/","link":"","permalink":"http://github.com/2018/04/11/【machine Learning】机器学习-1/","excerpt":"科学算法库的安装(linux) 1.安装Numpysudo apt-get install python-numpy 2.安装Scipysudo apt-get install python-numpy 3.Matplotlibsudo apt-get install tk-dev sudo apt-get install python-gtk2-dev sudo apt-get install python-pyside 4.spyder GUI环境sudo apt-get install spyder","text":"科学算法库的安装(linux) 1.安装Numpysudo apt-get install python-numpy 2.安装Scipysudo apt-get install python-numpy 3.Matplotlibsudo apt-get install tk-dev sudo apt-get install python-gtk2-dev sudo apt-get install python-pyside 4.spyder GUI环境sudo apt-get install spyder 上述安装完毕后，可以利用 1234567891011#! /usr/bin/pythonimport numpy as npimport matplotlib.pyplot as pltx = np.linspace(0,4*3.1415,100)y = np.sin(x)plt.figure(figsize=(8,4))plt.plot(x,y,label=\"$sin(x)$\",color=\"red\",linewidth=2)plt.legend()plt.show() 进行测试。若生成正弦曲线窗口，则配置完成. NumPy的基本操作 Numpy 的导入import numpy as np 这种写法在使用相关函数的时候需要写明是哪个包的，如: myZero = np.zeros([3,5]) 还可以导入包全局使用 from numpy import * NumPy 的基本操作 创建全0矩阵和全1矩阵 12myZero = zeros([n,m])myOne = ones([n,m]) 生成随机矩阵 1myRand = random.rand(n,m) # n 行 m 列的 0～1 之间的随机数矩阵 生成单位矩阵 1myEye = eye(n) # n * n 的单位阵 将一个数组转化为一个矩阵 1myMatrix = mat([[1,2,3],[4,5,6],[7,8,9]]) 矩阵所有元素求和 1S = sum(myMatrix) 矩阵各元素的乘积 1matrix = multiply(matrix1, matrix2) # matrix1 和 matrix2 对应元素相乘的矩阵 求矩阵的 n 次幂 1matrix = power(myMatrix, n) #生成一个矩阵，矩阵内部的元素是原矩阵对应元素的n次幂 矩阵的转置 12print matrix.T #打印转置后的矩阵，不改变原矩阵matrix.transpose() #同上 矩阵的其他操作 1234567[m, n] = shape(matrix) # m, n为矩阵的行列数myscl1 = matrix[0] # 矩阵的切片操作，取第一行myscl2 = matrix.T[0] # 矩阵的切片操作，取第一列mycpmat = matrix.copy() # 矩阵的复制print matrix1 &lt; matrix2 # 矩阵的比较，会逐一比较对应的每一个元素，并输出一个仅包含True, False 的相同大小的矩阵dot(m1,m2) #计算m1,m2的点积norm(v) #计算向量V的范数 Linalg线性代数库 矩阵的行列式 1print linalg.det(matrix) 矩阵的逆 1print linalg.inv(matrix) 矩阵的对称 1print matrix * matrix.T 矩阵的秩 1print linalg.matrix_rank(A) 可逆矩阵求解 1print linalg.solve(A,b.T) # 如果b已经是一列的就不用转置了 各类距离的python实现 各类距离会在后面说明 Euclidean Distance 123vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sqrt((vector1-vector2)*(vector1-vector2).T) Manhattan Distance 123vector1 = mat([1,2,3])vector2 = mat([4,5,6])print sum(abs(vector1-vector2)) Chebyshev Distance 123vector1 = mat([1,2,3])vector2 = mat([4,5,6])print abs(vector1-vector2).max() Cosine 12cosV12 = dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))print cosV12 Hamming Distance 123matV = mat([[1,1,0,1,0,1,0,0,1], [0,1,1,0,0,0,1,1,1]])smstr = nonzero(matV[0] - matV[1]);print shape(smstr[0])[1] Jaccard Distance 123import scipy.spatial.distance as distmatV = mat([[1,1,0,1,0,1,0,0,1], [0,1,1,0,0,0,1,1,1]])print dist.pdist(matV, 'jaccard') 机器学习的数学基础 范数 向量的范数可以简单、形象的理解为向量的长度，或者向量到坐标系原点的距离，或者相应空间内的两点之间的距离 向量的范数定义 : 向量的范数是一个函数 $ \\parallel x\\parallel $ ,满足非负性 $ \\parallel x\\parallel &gt; 0 $ , 齐次性 $ \\parallel cx\\parallel = \\mid c\\mid\\parallel x\\parallel $ ,三角不等式 $ \\parallel x+y\\parallel \\leq\\parallel x\\parallel +\\parallel y\\parallel $ 。 L1范数： $\\parallel x\\parallel $为 $ x $向量各个元素绝对值之和。L2范数： $\\parallel x\\parallel $为 $ x $向量各个元素平方和的开方，又称 Euclidean 范数或者 Frobenius 范数。Lp范数： $\\parallel x\\parallel $为 $ x $向量各个元素绝对值 $ p $次方和的 $ 1\\over p $ 次方L $\\infty $范数： $\\parallel x\\parallel $为 $ x $向量各个元素绝对值最大的那个元素，如下：$$ \\lim_{k\\to\\infty}\\left( \\sum_{i=1}^n\\mid p_i-q_i\\mid ^k\\right)^\\frac{1}{k}$$ Minkowski Distance (闵可夫斯基距离) 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义。两个n维向量 $ A(x_{11},x_{12},\\cdots,x_{1n}) $ 与 $ B(x_{21},x_{22},\\cdots,x_{2n}) $ 间的Minkowski距离定义为：$$ d_{12}=\\sqrt[p]{\\sum_{k=1}^n(x_{1k}-x_{2k})^p} $$其中p是一个变参数。 当 p=1 时，就是 Manhattan Distance (曼哈顿距离) 当 p=2 时，就是 Euclidean Distance (欧氏距离) 当 $ p\\to\\infty $ 时，就是 Chebyshev Distance (切比雪夫距离) Euclideam Distance 欧氏距离（L2范数）是最易于理解的一种距离计算方法，源于欧氏空间的两点距离公式两个n维向量 $ A(x_{11},x_{12},\\cdots,x_{1n}) $ 与 $ B(x_{21},x_{22},\\cdots,x_{2n}) $ 之间的欧氏距离：$$ d_{12}=\\sqrt{\\sum_{k=1}^n(x_{1k}-x_{2k})^2} $$表示为向量运算的形式：$$ d_{12}=\\sqrt{(A - B)(A - B)^T} $$ Manhattan Distance 曼哈顿距离（L1范数）可以理解为计算网格中两点路径的距离二维平面两点 $ A(x_1,y_1) $ 和 $ B(x_2,y_2) $ 间的曼哈顿距离:$$ d_{12}=\\mid x_1-x_2\\mid +\\mid y_1-y_2\\mid $$两个n维向量 $ A(x_{11},x_{12},\\cdots,x_{1n}) $ 与 $ B(x_{21},x_{22},\\cdots,x_{2n}) $ 之间的曼哈顿距离：$$ d_{12}=\\sum_{k=1}^n\\mid x_{1k}-x_{2k}\\mid $$ Chebyshev Distance 切比雪夫距离类似与棋盘上国王从一点到另一点移动的最少次数，即 $ max(\\mid x_1-x_2\\mid,\\mid y_1-y_2\\mid) $两个n维向量 $ A(x_{11},x_{12},\\cdots,x_{1n}) $ 与 $ B(x_{21},x_{22},\\cdots,x_{2n}) $ 之间的切比雪夫距离：$$ d_{12}=max_i(\\mid x_{1i}-x_{2i}\\mid) $$该公式的另一个等价公式：$$ d_{12}=\\lim_{k\\to\\infty}\\left(\\sum_{i=1}^n\\mid x_{1i}-x_{2i}\\mid^k\\right)^\\frac{1}{k} $$ Cosine 夹角余弦可以用来两个向量方向的差异，机器学习中借用这一概念来衡量样本之间的差异两个n维向量 $ A(x_{11},x_{12},\\cdots,x_{1n}) $ 与 $ B(x_{21},x_{22},\\cdots,x_{2n}) $ 之间的夹角余弦：$$ \\cos\\theta=\\frac{AB}{\\mid A\\mid\\mid B\\mid} $$即：$$ \\cos\\theta=\\frac{\\sum_{k=1}^nx_{1k}x_{2k}}{\\sqrt{\\sum_{k=1}^nx_{1k}^2}\\sqrt{\\sum_{k=1}^nx_{2k}^2}} $$ Hamming Distance 汉明距离的定义：两个等长字符串s1,s2,将其中一个变成另一个需要的最小替换次数。应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大） Jaccard Similarity Coefficient(杰卡德相似系数) 杰卡德相似系数：两个集合A,B的交集元素在并集元素中所占的比例，用符号 $ J(A,B) $ 表示$$ J(A,B)=\\frac{\\mid A\\cap B\\mid}{\\mid A\\cup B\\mid} $$杰卡德距：与杰卡德相似系数相反的概念：$$ J_\\delta(A,B)=1-J(A,B)=\\frac{\\mid A\\cup B\\mid-\\mid A\\cap B\\mid}{\\mid A\\cup B\\mid} $$ 特征间的相关性 相关系数与相关距离 相关系数： $$ \\rho_{XY}=\\frac{Cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}=\\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)}\\sqrt{D(Y)}} $$ 相关距离： $$ D_{XY}=1-\\rho_{XY} $$python实现： 12345678910featuremat = mat(...) # 初始化矩阵# 计算均值mv1 = mean(featuremat[0]) # 计算第一列的均值mv2 = mean(featuremat[1]) # 计算第二列的均值#计算两列的标准差dv1 = std(featuremat[0])dv2 = std(featuremat[1])corref = mean(multiply(featuremat[0]-mv1,featuremat[1]-mv2))/(dv1*dv2)print corref #输出相关系数print corrcoef(featuremat) #输出相关系数矩阵 马氏距离","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://github.com/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://github.com/tags/机器学习/"},{"name":"Python","slug":"Python","permalink":"http://github.com/tags/Python/"}]},{"title":"【Machine Learning】机器学习：简明入门指南","slug":"【Machine Learning】机器学习：简明入门指南","date":"2017-10-11T08:00:08.000Z","updated":"2018-04-11T06:07:49.209Z","comments":true,"path":"2017/10/11/【Machine Learning】机器学习：简明入门指南/","link":"","permalink":"http://github.com/2017/10/11/【Machine Learning】机器学习：简明入门指南/","excerpt":"本文是一篇转载自伯乐在线的译文，英文原文是这里：Machine Learning is Fun! — by Adam Geitgey 在听到人们谈论机器学习的时候，你是不是对它的涵义只有几个模糊的认识呢？你是不是已经厌倦了在和同事交谈时只能一直点头？让我们改变一下吧！ 本指南的读者对象是所有对机器学习有求知欲但却不知道如何开头的朋友。我猜很多人已经读过了“机器学习” ——维基百科词条，倍感挫折，以为没人能给出一个高层次的解释。本文就是你们想要的东西。 本文目标在于平易近人，这意味着文中有大量的概括。但是谁在乎这些呢？只要能让读者对于ML更感兴趣，任务也就完成了。","text":"本文是一篇转载自伯乐在线的译文，英文原文是这里：Machine Learning is Fun! — by Adam Geitgey 在听到人们谈论机器学习的时候，你是不是对它的涵义只有几个模糊的认识呢？你是不是已经厌倦了在和同事交谈时只能一直点头？让我们改变一下吧！ 本指南的读者对象是所有对机器学习有求知欲但却不知道如何开头的朋友。我猜很多人已经读过了“机器学习” ——维基百科词条，倍感挫折，以为没人能给出一个高层次的解释。本文就是你们想要的东西。 本文目标在于平易近人，这意味着文中有大量的概括。但是谁在乎这些呢？只要能让读者对于ML更感兴趣，任务也就完成了。 何为机器学习？机器学习这个概念认为，对于待解问题，你无需编写任何专门的程序代码，遗传算法（generic algorithms）能够在数据集上为你得出有趣的答案。对于遗传算法，不用编码，而是将数据输入，它将在数据之上建立起它自己的逻辑。 举个例子，有一类算法称为分类算法，它可以将数据划分为不同的组别。一个用来识别手写数字的分类算法，不用修改一行代码，就可以用来将电子邮件分为垃圾邮件和普通邮件。算法没变，但是输入的训练数据变了，因此它得出了不同的分类逻辑。 机器学习算法是个黑盒，可以重用来解决很多不同的分类问题。 “机器学习”是一个涵盖性术语，覆盖了大量类似的遗传算法。 两类机器学习算法你可以认为机器学习算法分为两大类：监督式学习（Supervised Learning）和非监督式学习（Unsupervised Learning）。两者区别很简单，但却非常重要。 监督式学习假设你是一名房产经纪，生意越做越大，因此你雇了一批实习生来帮你。但是问题来了——你可以看一眼房子就知道它到底值多少钱，实习生没有经验，不知道如何估价。 为了帮助你的实习生（也许是为了解放你自己去度个假），你决定写个小软件，可以根据房屋大小、地段以及类似房屋的成交价等因素来评估你所在地区房屋的价值。 你把3个月来城里每笔房屋交易都写了下来，每一单你都记录了一长串的细节——卧室数量、房屋大小、地段等等。但最重要的是，你写下了最终的成交价： 这是我们的“训练数据”: 我们要利用这些训练数据来编写一个程序来估算该地区其他房屋的价值： 这就称为监督式学习。你已经知道每一栋房屋的售价，换句话说，你知道问题的答案，并可以反向找出解题的逻辑。 为了编写软件，你将包含每一套房产的训练数据输入你的机器学习算法。算法尝试找出应该使用何种运算来得出价格数字。 这就像是算术练习题，算式中的运算符号都被擦去了：天哪！一个阴险的学生将老师答案上的算术符号全擦去了。 看了这些题，你能明白这些测验里面是什么样的数学问题吗？你知道，你应该对算式左边的数字“做些什么”以得出算式右边的答案。 在监督式学习中，你是让计算机为你算出数字间的关系。而一旦你知道了解决这类特定问题所需要的数学方法后，你就可以解答同类的其它问题了。 非监督式学习让我们回到开头那个房地产经纪的例子。要是你不知道每栋房子的售价怎么办？即使你所知道的只是房屋的大小、位置等信息，你也可以搞出很酷的花样。这就是所谓的非监督式学习。 即使你不是想去预测未知的数据（如价格），你也可以运用机器学习完成一些有意思的事。 这就有点像有人给你一张纸，上面列出了很多数字，然后对你说:“我不知道这些数字有什么意义，也许你能从中找出规律或是能将它们分类，或是其它什么-祝你好运！” 你该怎么处理这些数据呢？首先，你可以用个算法自动地从数据中划分出不同的细分市场。也许你会发现大学附近的买房者喜欢户型小但卧室多的房子，而郊区的买房者偏好三卧室的大户型。这些信息可以直接帮助你的营销。 你还可以作件很酷的事，自动找出房价的离群数据，即与其它数据迥异的值。这些鹤立鸡群的房产也许是高楼大厦，而你可以将最优秀的推销员集中在这些地区，因为他们的佣金更高。 本文余下部分我们主要讨论监督式学习，但这并不是因为非监督式学习用处不大或是索然无味。实际上，随着算法改良，不用将数据和正确答案联系在一起，因此非监督式学习正变得越来越重要。 老学究请看:还有很多其它种类的机器学习算法。但初学时这样理解不错了。 太酷了，但是评估房价真能被看作“学习”吗？作为人类的一员，你的大脑可以应付绝大多数情况，并且没有任何明确指令也能够学习如何处理这些情况。如果你做房产经纪时间很长，你对于房产的合适定价、它的最佳营销方式以及哪些客户会感兴趣等等都会有一种本能般的“感觉”。强人工智能（Strong AI）研究的目标就是要能够用计算机复制这种能力。 但是目前的机器学习算法还没有那么好——它们只能专注于非常特定的、有限的问题。也许在这种情况下，“学习”更贴切的定义是“在少量范例数据的基础上找出一个等式来解决特定的问题”。 不幸的是，“机器在少量范例数据的基础上找出一个等式来解决特定的问题”这个名字太烂了。所以最后我们用“机器学习”取而代之。 当然，要是你是在50年之后来读这篇文章，那时我们已经得出了强人工智能算法，而本文看起来就像个老古董。未来的人类，你还是别读了，叫你的机器仆人给你做份三明治吧。 让我们写代码吧!前面例子中评估房价的程序，你打算怎么写呢？往下看之前，先思考一下吧。 如果你对机器学习一无所知，很有可能你会尝试写出一些基本规则来评估房价，如下： 123456789101112131415161718192021222324252627def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # In my area, the average house costs $200 per sqft price_per_sqft = 200 if neighborhood == \"hipsterton\": # but some areas cost a bit more price_per_sqft = 400 elif neighborhood == \"skid row\": # and some areas cost less price_per_sqft = 100 # start with a base price estimate based on how big the place is price = price_per_sqft * sqft # now adjust our estimate based on the number of bedrooms if num_of_bedrooms == 0: # Studio apartments are cheap price = price — 20000 else: # places with more bedrooms are usually # more valuable price = price + (num_of_bedrooms * 1000) return price 假如你像这样瞎忙几个小时，也许会取得一点成效，但是你的程序永不会完美，而且当价格变化时很难维护。 如果能让计算机找出实现上述函数功能的办法，这样岂不更好？只要返回的房价数字正确，谁会在乎函数具体干了些什么呢？ 1234def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = &lt;computer, plz do some math for me&gt; return price 考虑这个问题的一种角度是将房价看做一碗美味的汤，而汤中成分就是卧室数、面积和地段。如果你能算出每种成分对最终的价格有多大影响，也许就能得到各种成分混合起来形成最终价格的具体比例。 这样可以将你最初的程序（全是疯狂的if else语句）简化成类似如下的样子： 12345678910111213141516def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # a little pinch of this price += num_of_bedrooms * .841231951398213 # and a big pinch of that price += sqft * 1231.1231231 # maybe a handful of this price += neighborhood * 2.3242341421 # and finally, just a little extra salt for good measure price += 201.23432095 return price 请注意那些用粗体标注的神奇数字——.841231951398213, 1231.1231231,2.3242341421, 和201.23432095。它们称为权重。如果我们能找出对每栋房子都适用的完美权重，我们的函数就能预测所有的房价！ 找出最佳权重的一种笨办法如下所示： 步骤1：首先，将每个权重都设为1.0： 12345678910111213141516def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # a little pinch of this price += num_of_bedrooms * 1.0 # and a big pinch of that price += sqft * 1.0 # maybe a handful of this price += neighborhood * 1.0 # and finally, just a little extra salt for good measure price += 1.0 return price 步骤2：将每栋房产带入你的函数运算，检验估算值与正确价格的偏离程度： 运用你的程序预测房屋价格。 例如：上表中第一套房产实际成交价为25万美元，你的函数估价为17.8万，这一套房产你就差了7.2万。 再将你的数据集中的每套房产估价偏离值平方后求和。假设数据集中有500套房产交易，估价偏离值平方求和总计为86,123,373美元。这就反映了你的函数现在的“正确”程度。 现在，将总计值除以500，得到每套房产的估价偏离平均值。将这个平均误差值称为你函数的代价。 如果你能调整权重使得这个代价变为0，你的函数就完美了。它意味着，根据输入的数据，你的程序对每一笔房产交易的估价都是分毫不差。而这就是我们的目标——尝试不同的权重值以使代价尽可能的低。 步骤3：不断重复步骤2，尝试所有可能的权重值组合。哪一个组合使得代价最接近于0，它就是你要使用的，你只要找到了这样的组合，问题就得到了解决! 思想扰动时间这太简单了，对吧？想一想刚才你做了些什么。你取得了一些数据，将它们输入至三个通用的简单步骤中，最后你得到了一个可以对你所在区域的房屋进行估价的函数。房价网，要当心咯！但是下面的事实可能会扰乱你的思想： 1.过去40年来，很多领域（如语言学/翻译学）的研究表明，这种通用的“搅动数据汤”（我编造的词）式的学习算法已经胜过了需要利用真人明确规则的方法。机器学习的“笨”办法最终打败了人类专家。 2.你最后写出的函数真是笨，它甚至不知道什么是“面积”和“卧室数”。它知道的只是搅动，改变数字来得到正确的答案。 3.很可能你都不知道为何一组特殊的权重值能起效。所以你只是写出了一个你实际上并不理解却能证明的函数。 4.试想一下，你的程序里没有类似“面积”和“卧室数”这样的参数，而是接受了一组数字。假设每个数字代表了你车顶安装的摄像头捕捉的画面中的一个像素，再将预测的输出不称为“价格”而是叫做“方向盘转动度数”，这样你就得到了一个程序可以自动操纵你的汽车了！ 太疯狂了，对吧？ 步骤3中的“尝试每个数字”怎么回事？好吧，当然你不可能尝试所有可能的权重值来找到效果最好的组合。那可真要花很长时间，因为要尝试的数字可能无穷无尽。 为避免这种情况，数学家们找到了很多聪明的办法（比如Gradient descent算法）来快速找到优秀的权重值，而不需要尝试过多。下面是其中一种： 首先，写出一个简单的等式表示前述步骤2，这是你的代价函数： 接着，让我们将这同一个等式用机器学习的数学术语（现在你可以忽略它们）进行重写： θ表示当前的权重值。 J(θ) 意为“当前权重值对应的代价”。 这个等式表示我们的估价程序在当前权重值下偏离程度的大小。如果将所有赋给卧室数和面积的可能权重值以图形形式显示，我们会得到类似下图的图表： 代价函数的图形像一支碗。纵轴表示代价。 图中蓝色的最低点就是代价最低的地方——即我们的程序偏离最小。最高点意味着偏离最大。所以，如果我们能找到一组权重值带领我们到达图中的最低点，我们就找到了答案！ 因此，我们只需要调整权重值使我们在图上能向着最低点“走下坡路”。如果对于权重的细小调节能一直使我们保持向最低点移动，那么最终我们不用尝试太多权重值就能到达那里。 如果你还记得一点微积分的话，你也许记得如果你对一个函数求导，结果会告诉你函数在任一点的斜率。换句话说，对于图上给定一点，它告诉我们那条路是下坡路。我们可以利用这一点朝底部进发。 所以，如果我们对代价函数关于每一个权重求偏导，那么我们就可以从每一个权重中减去该值。这样可以让我们更加接近山底。一直这样做，最终我们将到达底部，得到权重的最优值。（读不懂？不用担心，接着往下读）。 这种找出最佳权重的办法被称为批量梯度下降，上面是对它的高度概括。如果想搞懂细节，不要害怕，继续深入下去吧。 当你使用机器学习算法库来解决实际问题，所有这些都已经为你准备好了。但明白一些具体细节总是有用的。 还有什么你随便就略过了？上面我描述的三步算法被称为多元线性回归。你估算等式是在求一条能够拟合所有房价数据点的直线。然后，你再根据房价在你的直线上可能出现的位置用这个等式来估算从未见过的房屋的价格。这个想法威力强大，可以用它来解决“实际”问题。 但是，我为你展示的这种方法可能在简单的情况下有效，它不会在所有情况下都有用。原因之一是因为房价不会一直那么简单地跟随一条连续直线。 但是，幸运的是，有很多办法来处理这种情况。对于非线性数据，很多其他类型的机器学习算法可以处理（如神经网络或有核向量机）。还有很多方法运用线性回归更灵活，想到了用更复杂的线条来拟合。在所有的情况中，寻找最优权重值这一基本思路依然适用。 还有，我忽略了过拟合的概念。很容易碰上这样一组权重值，它们对于你原始数据集中的房价都能完美预测，但对于原始数据集之外的任何新房屋都预测不准。这种情况的解决之道也有不少（如正则化以及使用交叉验证数据集）。学会如何处理这一问题对于顺利应用机器学习至关重要。 换言之，基本概念非常简单，要想运用机器学习得到有用的结果还需要一些技巧和经验。但是，这是每个开发者都能学会的技巧。 机器学习法力无边吗？一旦你开始明白机器学习技术很容易应用于解决貌似很困难的问题（如手写识别），你心中会有一种感觉，只要有足够的数据，你就能够用机器学习解决任何问题。只需要将数据输入进去，就能看到计算机变戏法一样找出拟合数据的等式。 但是很重要的一点你要记住，机器学习只能对用你占有的数据实际可解的问题才适用。 例如，如果你建立了一个模型来根据每套房屋内盆栽数量来预测房价，它就永远不会成功。房屋内盆栽数量和房价之间没有任何的关系。所以，无论它怎么去尝试，计算机也推导不出两者之间的关系。 你只能对实际存在的关系建模。 怎样深入学习机器学习我认为，当前机器学习的最大问题是它主要活跃于学术界和商业研究组织中。对于圈外想要有个大体了解而不是想成为专家的人们，简单易懂的学习资料不多。但是这一情况每一天都在改善。 吴恩达教授（Andrew Ng）在Coursera上的机器学习免费课程非常不错。我强烈建议由此入门。任何拥有计算机科学学位、还能记住一点点数学的人应该都能理解。 另外，你还可以下载安装SciKit-Learn，用它来试验成千上万的机器学习算法。它是一个python框架，对于所有的标准算法都有“黑盒”版本。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://github.com/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://github.com/tags/Machine-Learning/"}]},{"title":"我的第一篇文章","slug":"我的第一篇文章","date":"2017-09-22T12:14:15.000Z","updated":"2017-09-24T14:49:41.629Z","comments":true,"path":"2017/09/22/我的第一篇文章/","link":"","permalink":"http://github.com/2017/09/22/我的第一篇文章/","excerpt":"","text":"我的第一次博客","categories":[{"name":"技术","slug":"技术","permalink":"http://github.com/categories/技术/"},{"name":"javascript","slug":"技术/javascript","permalink":"http://github.com/categories/技术/javascript/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://github.com/tags/前端/"},{"name":"后端","slug":"后端","permalink":"http://github.com/tags/后端/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-09-22T11:48:15.570Z","updated":"2018-04-12T03:30:36.790Z","comments":true,"path":"2017/09/22/hello-world/","link":"","permalink":"http://github.com/2017/09/22/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}